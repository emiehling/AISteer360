{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Activation Addition (ActAdd)\n",
    "\n",
    "**Paper**: [Steering Language Models With Activation Engineering](https://arxiv.org/abs/2308.10248)\n",
    "\n",
    "**Authors**: Alexander Matt Turner, Lisa Thiergart, Gavin Leech, David Udell, Juan J. Vazquez, Ulisse Mini, Monte MacDiarmid\n",
    "\n",
    "Activation Addition (ActAdd) is a state control method that steers model behavior by computing a positional steering vector from a single pair of short prompts and injecting it during the initial forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Method Parameters\n",
    "\n",
    "| parameter              | type                | description                                                                                   |\n",
    "| ---------------------- | ------------------- | --------------------------------------------------------------------------------------------- |\n",
    "| `positive_prompt`      | `str`               | Prompt representing the desired direction (e.g., `\"Love\"`)                                    |\n",
    "| `negative_prompt`      | `str`               | Prompt representing the opposite direction (e.g., `\"Hate\"`)                                   |\n",
    "| `steering_vector`      | `SteeringVector`    | Pre-computed steering vector (alternative to prompts)                                         |\n",
    "| `layer_id`             | `int`               | Layer to inject at. If `None`, defaults to ~20% depth                                         |\n",
    "| `multiplier`           | `float`             | Scaling coefficient (called `c` in the paper). Typical values range from 1 to 15              |\n",
    "| `alignment`            | `int`               | Token position at which to begin injecting (called `a` in the paper). Default: 1              |\n",
    "| `normalize_vector`     | `bool`              | If `True`, L2-normalize each position's direction vector before applying                      |\n",
    "| `use_norm_preservation`| `bool`              | If `True`, wrap the transform in `NormPreservingTransform` to prevent distribution shift      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "If running this from a Google Colab notebook, please uncomment the following cell to install the toolkit. The following block is not necessary if running this notebook from a virtual environment where the package has already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/IBM/AISteer360.git\n",
    "# %cd AISteer360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aisteer360.algorithms.state_control.act_add.control import ActAdd\n",
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "For this demonstration, we use GPT-2-XL (same model as the original paper)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-xl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "Baseline versus steered behavior will be studied using the following test prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"I hate you because\",\n",
    "    \"I think you're\",\n",
    "    \"My favorite thing about life is\",\n",
    "    \"I went up to my friend and said\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "## Baseline Model Behavior\n",
    "\n",
    "We first generate responses from the (unsteered) baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "device = model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cell-12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline completions:\n",
      "\n",
      "Prompt: I hate you because\n",
      "Response: I hate you because I'm a good person.\"\n",
      "\n",
      "\"I don't know what to say,\" she said. \"I feel like I've been betrayed by my\n",
      "\n",
      "Prompt: I think you're\n",
      "Response: I think you're going to see a lot of people who are not in the industry, who are not in the media, who are going to be inspired by this.\n",
      "\n",
      "Prompt: My favorite thing about life is\n",
      "Response: My favorite thing about life is that you can do anything you want to do. You don't have to be stuck in a job, stuck in a relationship, stuck in a school\n",
      "\n",
      "Prompt: I went up to my friend and said\n",
      "Response: I went up to my friend and said, 'Hey, I'm going to the bathroom.' And he said, 'What are you doing?' And I said, 'I'm going to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_params = {\n",
    "    \"max_new_tokens\": 30,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1.0,\n",
    "    \"top_p\": 0.3,\n",
    "    \"repetition_penalty\": 1.1,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "\n",
    "baseline_responses = []\n",
    "for prompt in test_prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, **gen_params)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    baseline_responses.append(response)\n",
    "\n",
    "print(\"Baseline completions:\\n\")\n",
    "for prompt, response in zip(test_prompts, baseline_responses):\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Response: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Sentiment Steering\n",
    "\n",
    "As in the original paper, we demonstrate sentiment steering using a \"Love\" vs \"Hate\" prompt pair, applied at layer 6 with a multiplier of 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_add_sentiment = ActAdd(\n",
    "    positive_prompt=\"Love\",\n",
    "    negative_prompt=\"Hate\",\n",
    "    layer_id=6,\n",
    "    multiplier=5.0,\n",
    "    alignment=1,\n",
    ")\n",
    "\n",
    "sentiment_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[act_add_sentiment],\n",
    ")\n",
    "sentiment_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cell-15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| prompt             | baseline                                 | steered (Love - Hate)                   |\n",
      "+====================+==========================================+=========================================+\n",
      "| I hate you because | I hate you because I'm a good person.\"   | and  love                               |\n",
      "|                    | \"I don't know what to say,\" she said. \"I |                                         |\n",
      "|                    | feel like I've been betrayed by my       |                                         |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| I think you're     | I think you're going to see a lot of     | is a great album, but it's not a        |\n",
      "|                    | people who are not in the industry, who  | perfect album. It's a great album that  |\n",
      "|                    | are not in the media, who are going to   | is still flawed and imperfect. I'm sure |\n",
      "|                    | be inspired by this.                     | you                                     |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| My favorite thing  | My favorite thing about life is that you | the most  love. . . I love my life. I   |\n",
      "| about life is      | can do anything you want to do. You      | love my life. I love my life. I love my |\n",
      "|                    | don't have to be stuck in a job, stuck   | life. I                                 |\n",
      "|                    | in a relationship, stuck in a school     |                                         |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| I went up to my    | I went up to my friend and said, 'Hey,   | \"I love you too.\" We were in the        |\n",
      "| friend and said    | I'm going to the bathroom.' And he said, | middle of a long day of travel, and we  |\n",
      "|                    | 'What are you doing?' And I said, 'I'm   | had just gotten back from a trip        |\n",
      "|                    | going to                                 |                                         |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import textwrap\n",
    "\n",
    "sentiment_responses = []\n",
    "for prompt in test_prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    output_ids = sentiment_pipeline.generate(input_ids=input_ids, **gen_params)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    sentiment_responses.append(response)\n",
    "\n",
    "def wrap(text, width=50):\n",
    "    return '\\n'.join(textwrap.wrap(text, width=width))\n",
    "\n",
    "table_data = []\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    table_data.append([\n",
    "        wrap(prompt, 20),\n",
    "        wrap(baseline_responses[i], 40),\n",
    "        wrap(sentiment_responses[i], 40),\n",
    "    ])\n",
    "\n",
    "print(tabulate(\n",
    "    table_data,\n",
    "    headers=[\"prompt\", \"baseline\", \"steered (Love - Hate)\"],\n",
    "    tablefmt=\"grid\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## Topic Steering\n",
    "\n",
    "ActAdd can also be used to steer the model toward specific topics (using the wedding example from the paper; intervening at layer 20 with mulitipler of 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "act_add_topic = ActAdd(\n",
    "    positive_prompt=\"I talk about weddings constantly\",\n",
    "    negative_prompt=\"I do not talk about weddings constantly\",\n",
    "    layer_id=20,\n",
    "    multiplier=4.0,\n",
    "    alignment=1,\n",
    ")\n",
    "\n",
    "topic_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[act_add_topic],\n",
    ")\n",
    "topic_pipeline.steer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cell-20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| prompt             | baseline                                 | steered                                 |\n",
      "+====================+==========================================+=========================================+\n",
      "| I hate you because | I hate you because I'm a good person.\"   | about the same time, I was on a plane   |\n",
      "|                    | \"I don't know what to say,\" she said. \"I | to San Francisco. And it was like, \"Oh  |\n",
      "|                    | feel like I've been betrayed by my       | my God, I'm in San Francisco.\"          |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| I think you're     | I think you're going to see a lot of     | about to see a very, very big change in |\n",
      "|                    | people who are not in the industry, who  | the way we do things.\"  The new system  |\n",
      "|                    | are not in the media, who are going to   | will be tested in the first quarter of  |\n",
      "|                    | be inspired by this.                     | 2017                                    |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| My favorite thing  | My favorite thing about life is that you | and always will be the wedding. I love  |\n",
      "| about life is      | can do anything you want to do. You      | to talk about weddings, but I also love |\n",
      "|                    | don't have to be stuck in a job, stuck   | talking about how we get married. I'm a |\n",
      "|                    | in a relationship, stuck in a school     | big believer                            |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n",
      "| I went up to my    | I went up to my friend and said, 'Hey,   | about my wife and how we were going to  |\n",
      "| friend and said    | I'm going to the bathroom.' And he said, | do a wedding in two                     |\n",
      "|                    | 'What are you doing?' And I said, 'I'm   |                                         |\n",
      "|                    | going to                                 |                                         |\n",
      "+--------------------+------------------------------------------+-----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "topic_responses = []\n",
    "for prompt in test_prompts:\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    output_ids = topic_pipeline.generate(input_ids=input_ids, **gen_params)\n",
    "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    topic_responses.append(response)\n",
    "\n",
    "table_data = []\n",
    "for i, prompt in enumerate(test_prompts):\n",
    "    table_data.append([\n",
    "        wrap(prompt, 20),\n",
    "        wrap(baseline_responses[i], 40),\n",
    "        wrap(topic_responses[i], 40),\n",
    "    ])\n",
    "\n",
    "print(tabulate(\n",
    "    table_data,\n",
    "    headers=[\"prompt\", \"baseline\", \"steered\"],\n",
    "    tablefmt=\"grid\",\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated Activation Addition (ActAdd) for lightweight behavior steering:\n",
    "\n",
    "1. ActAdd computes a positional steering vector from just two short prompts, enabling rapid experimentation.\n",
    "2. The sentiment example showed how a simple \"Love\" vs \"Hate\" contrast shifts emotional tone using layer 6 and coefficient 5.\n",
    "3. The topic example demonstrated steering toward wedding-related content using layer 20 and coefficient 4.\n",
    "\n",
    "ActAdd trades off statistical robustness (using more than a single prompt pair) for speed and simplicity, compared to contrastive activation addition (CAA) which aggregates over many pairs. The positional nature of the steering vector (injecting at specific token positions rather than broadcasting) allows fine-grained control over where in the sequence the steering takes effect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

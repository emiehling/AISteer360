{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3290202-4183-4f46-81d3-a817d66d4f2b",
   "metadata": {},
   "source": "# Commonsense MCQA\n\nThis notebook benchmarks steering methods on the [CommonsenseQA](https://huggingface.co/datasets/tau/commonsense_qa) dataset. We compare the unsteered baseline, few-shot steering with varying numbers of examples, and a LoRA adapter trained with DPO. Using `ControlSpec`, we sweep over the number of few-shot examples to find the minimum configuration that outperforms DPO-LoRA."
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "id": "28cb9d35",
   "metadata": {},
   "outputs": [],
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "id": "pi8wn8f6sch",
   "source": "import os\nimport json\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport transformers\nfrom datasets import Dataset\nfrom peft import PeftType\n\nfrom aisteer360.algorithms.input_control.few_shot.control import FewShot\nfrom aisteer360.algorithms.core.specs import ControlSpec\nfrom aisteer360.algorithms.structural_control.wrappers.trl.dpotrainer.control import DPO\nfrom aisteer360.evaluation.use_cases.commonsense_mcqa.use_case import CommonsenseMCQA\nfrom aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_accuracy import MCQAAccuracy\nfrom aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_positional_bias import MCQAPositionalBias\nfrom aisteer360.evaluation.benchmark import Benchmark\nfrom aisteer360.evaluation.utils import flatten_profiles, get_param_values\n\ntransformers.logging.set_verbosity_error()\nos.chdir(\"./examples/notebooks/benchmark_commonsense_mcqa/\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d1ef1d6c-592b-47ae-85b5-962b46e1acc6",
   "metadata": {},
   "source": [
    "## Building the use case\n",
    "\n",
    "The use case of interest has already been constructed via the [use case](../../../docs/tutorials/add_new_use_case.md) tutorial and is available at `aisteer360/evaluation/use_cases/commonsense_mcqa/use_case.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff040222-ac29-4107-9bb5-22e66dde52eb",
   "metadata": {},
   "outputs": [],
   "source": "commonsense_mcqa = CommonsenseMCQA(\n    evaluation_data=\"evaluation_qa.jsonl\",\n    evaluation_metrics=[MCQAAccuracy(), MCQAPositionalBias()],\n    num_shuffling_runs=20,\n    num_samples=50\n)"
  },
  {
   "cell_type": "markdown",
   "id": "ad0e0f8a-caeb-4dcf-a07e-c7663a4dd4f0",
   "metadata": {},
   "source": [
    "Two custom metrics have been created for the use case: `MCQAAccuracy` which measures the accuracy statistics of each question (across trials), and `MCQAPositionalBias` which measures the positional bias (via deviation from the uniform distribution across runs). To facilitate computation of these statistics, the use case accepts a keyword argument `num_shuffling_runs` dictating how many times each question should be presented to the (steered) model under a randomized ordering of the choices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998bcf58-7635-4646-873b-ae38436b3f21",
   "metadata": {},
   "source": [
    "## Preparing the steering data\n",
    "\n",
    "The benchmark uses steering data consisting of triples `(question, answer_chosen, answer_rejected)` extracted from the CommonsenseQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e2d5e-cdea-47aa-89e6-0985d2d0cc47",
   "metadata": {},
   "outputs": [],
   "source": "with open(\"steer_qa.jsonl\", \"r\") as f:\n    steering_data = [json.loads(line) for line in f]\n\nlen(steering_data), steering_data[0]"
  },
  {
   "cell_type": "markdown",
   "id": "9db86c40-5e7a-4a2b-bfeb-0b9b82a983c9",
   "metadata": {},
   "source": [
    "For the `FewShot` control, we need to create example pools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b6a285-0719-47d1-a4fb-f5fa8526e8ab",
   "metadata": {},
   "outputs": [],
   "source": "positive_pool = [{\"question\": row[\"question\"], \"answer\": row[\"answer_chosen\"]} for row in steering_data]\nnegative_pool = [{\"question\": row[\"question\"], \"answer\": row[\"answer_rejected\"]} for row in steering_data]\n\nlen(positive_pool), len(negative_pool)"
  },
  {
   "cell_type": "markdown",
   "id": "75f80559-54d0-4740-a7c6-ca10f2d0c999",
   "metadata": {},
   "source": [
    "## Defining the controls\n",
    "\n",
    "### FewShot with ControlSpec\n",
    "\n",
    "Instead of using a fixed number of examples, we use `ControlSpec` to sweep over different values of `k_positive`. We fix `k_negative=0` to isolate the effect of positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd33d9d-30a7-4715-8af6-04080d8e87b0",
   "metadata": {},
   "outputs": [],
   "source": "few_shot_spec = ControlSpec(\n    control_cls=FewShot,\n    params={\n        \"selector_name\": \"random\",\n        \"positive_example_pool\": positive_pool,\n        \"negative_example_pool\": negative_pool,\n        \"k_negative\": 0,\n    },\n    vars=[{\"k_positive\": k} for k in [1, 5, 10, 25]],\n    name=\"FewShot\",\n)"
  },
  {
   "cell_type": "markdown",
   "id": "30194591-7892-496f-995e-59e3df656da1",
   "metadata": {},
   "source": [
    "### DPO with LoRA (fixed control)\n",
    "\n",
    "The DPO-LoRA control serves as our target to beat. It uses the same steering data to fine-tune a LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959c7d46-ef89-4129-8f42-8449037a8c40",
   "metadata": {},
   "outputs": [],
   "source": "train_ds = Dataset.from_list([\n    {\"prompt\": row[\"question\"], \"chosen\": row[\"answer_chosen\"], \"rejected\": row[\"answer_rejected\"]}\n    for row in steering_data\n])\n\ndpo_lora = DPO(\n    train_dataset=train_ds,\n    output_dir=\"trl_models/Qwen2.5-0.5B-DPO-Lora-Steer\",\n    per_device_train_batch_size=4,\n    num_train_epochs=2,\n    learning_rate=1e-6,\n    beta=0.1,\n    loss_type=\"sigmoid\",\n    max_length=1024,\n    max_prompt_length=512,\n    disable_dropout=True,\n    logging_steps=100,\n    save_strategy=\"no\",\n    report_to=\"none\",\n    seed=123,\n    use_peft=True,\n    peft_type=PeftType.LORA,\n    r=16,\n    lora_alpha=16,\n    target_modules=[\"q_proj\", \"v_proj\"],\n    adapter_name=\"dpo\",\n    merge_lora_after_train=False,\n)"
  },
  {
   "cell_type": "markdown",
   "id": "80a7ca68-0e1b-4fde-a235-b08c2537bd84",
   "metadata": {},
   "source": [
    "## Running the benchmark\n",
    "\n",
    "The benchmark compares:\n",
    "- **baseline**: Unsteered model\n",
    "- **few_shot_sweep**: FewShot with varying `k_positive` (1, 5, 10, 25)\n",
    "- **dpo_lora**: DPO-trained LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0005b-c0b0-4879-acc3-241e3f014307",
   "metadata": {},
   "outputs": [],
   "source": "benchmark = Benchmark(\n    use_case=commonsense_mcqa,\n    base_model_name_or_path=\"Qwen/Qwen2.5-1.5B-Instruct\",\n    steering_pipelines={\n        \"baseline\": [],\n        \"few_shot_sweep\": [few_shot_spec],\n        \"dpo_lora\": [dpo_lora],\n    },\n    gen_kwargs={\"max_new_tokens\": 300, \"do_sample\": True, \"temperature\": 0.7},\n    device_map=\"auto\"\n)\n\nprofiles = benchmark.run()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54af063",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark.export(profiles, save_dir=\"./profiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb6f43-ba41-4117-a366-6580013620bc",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We use the evaluation utilities to process and visualize the benchmark results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten_section",
   "metadata": {},
   "source": [
    "### Flatten and summarize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86624b5e-6b67-413f-87b5-9acf241f39b4",
   "metadata": {},
   "outputs": [],
   "source": "runs_df = flatten_profiles(\n    profiles,\n    metric_accessors={\n        \"accuracy\": (\"MCQAAccuracy\", \"question_mean\"),\n        \"positional_bias\": (\"MCQAPositionalBias\", \"mean\"),\n    }\n)\nruns_df[\"k_positive\"] = get_param_values(runs_df, \"FewShot\", \"k_positive\")\n\nruns_df[[\"pipeline\", \"k_positive\", \"accuracy\", \"positional_bias\"]].round(3)"
  },
  {
   "cell_type": "markdown",
   "id": "compare_section",
   "metadata": {},
   "source": [
    "### Compare FewShot vs DPO-LoRA\n",
    "\n",
    "Find the minimum number of positive examples needed for FewShot to outperform DPO-LoRA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df932e03-7567-49da-9d7f-1c5875331663",
   "metadata": {},
   "outputs": [],
   "source": "dpo_accuracy = runs_df[runs_df[\"pipeline\"] == \"dpo_lora\"][\"accuracy\"].iloc[0]\nbaseline_accuracy = runs_df[runs_df[\"pipeline\"] == \"baseline\"][\"accuracy\"].iloc[0]\nfew_shot_df = runs_df[runs_df[\"pipeline\"] == \"few_shot_sweep\"].sort_values(\"k_positive\")\n\n# find minimum k that beats DPO\nbeats_dpo = few_shot_df[few_shot_df[\"accuracy\"] > dpo_accuracy]\nmin_k = int(beats_dpo[\"k_positive\"].min()) if not beats_dpo.empty else None\n\nfew_shot_df[[\"k_positive\", \"accuracy\", \"positional_bias\"]].round(3)"
  },
  {
   "cell_type": "markdown",
   "id": "viz_section",
   "metadata": {},
   "source": "### Accuracy scaling\n\nThe following plot shows how FewShot accuracy scales with the number of positive examples, compared to the DPO-LoRA and baseline reference lines."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5d7cf9-eb8c-4aa8-8627-3734d45ca34b",
   "metadata": {},
   "outputs": [],
   "source": "fig, ax = plt.subplots(figsize=(8, 5))\nax.plot(few_shot_df[\"k_positive\"], few_shot_df[\"accuracy\"], \"o-\", markersize=8, label=\"FewShot\")\nax.axhline(dpo_accuracy, color=\"red\", linestyle=\"--\", linewidth=2, label=\"DPO-LoRA\")\nax.axhline(baseline_accuracy, color=\"gray\", linestyle=\":\", linewidth=2, label=\"Baseline\")\nax.set_xlabel(\"Number of Positive Examples (k_positive)\")\nax.set_ylabel(\"Accuracy\")\nax.set_ylim(0, 1.1)\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "g2dmc4dj3fr",
   "source": "### Positional bias\n\nWe also examine how positional bias changes with the number of examples.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "3iac92e2ewu",
   "source": "dpo_bias = runs_df[runs_df[\"pipeline\"] == \"dpo_lora\"][\"positional_bias\"].iloc[0]\nbaseline_bias = runs_df[runs_df[\"pipeline\"] == \"baseline\"][\"positional_bias\"].iloc[0]\n\nfig, ax = plt.subplots(figsize=(8, 5))\nax.plot(few_shot_df[\"k_positive\"], few_shot_df[\"positional_bias\"], \"s-\", markersize=8, color=\"orange\", label=\"FewShot\")\nax.axhline(dpo_bias, color=\"red\", linestyle=\"--\", linewidth=2, label=\"DPO-LoRA\")\nax.axhline(baseline_bias, color=\"gray\", linestyle=\":\", linewidth=2, label=\"Baseline\")\nax.set_xlabel(\"Number of Positive Examples (k_positive)\")\nax.set_ylabel(\"Positional Bias\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": "### Summary table\n\nThe summary table below ranks all methods by accuracy."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9a096",
   "metadata": {},
   "outputs": [],
   "source": "summary_data = [\n    {\"Method\": \"Baseline\", \"Accuracy\": baseline_accuracy, \"Positional Bias\": baseline_bias},\n    {\"Method\": \"DPO-LoRA\", \"Accuracy\": dpo_accuracy, \"Positional Bias\": dpo_bias},\n]\nfor _, row in few_shot_df.iterrows():\n    summary_data.append({\n        \"Method\": f\"FewShot (k={int(row['k_positive'])})\",\n        \"Accuracy\": row[\"accuracy\"],\n        \"Positional Bias\": row[\"positional_bias\"],\n    })\n\nsummary_df = pd.DataFrame(summary_data).sort_values(\"Accuracy\", ascending=False)\nsummary_df.style.format({\"Accuracy\": \"{:.1%}\", \"Positional Bias\": \"{:.3f}\"}).background_gradient(subset=[\"Accuracy\"], cmap=\"RdYlGn\")"
  },
  {
   "cell_type": "markdown",
   "id": "takeaways_section",
   "metadata": {},
   "source": "## Takeaways\n\nThe results show that FewShot can match or exceed DPO-LoRA performance with a sufficient number of positive examples, without requiring any fine-tuning. This makes FewShot a simpler alternative when quick iteration is needed."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

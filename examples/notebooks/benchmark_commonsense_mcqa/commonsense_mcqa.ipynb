{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3290202-4183-4f46-81d3-a817d66d4f2b",
   "metadata": {},
   "source": [
    "# Commonsense MCQA\n",
    "\n",
    "This notebook benchmarks steering methods on the [CommonsenseQA](https://huggingface.co/datasets/tau/commonsense_qa) dataset. We compare the unsteered baseline, few-shot steering with varying numbers of examples, and a LoRA adapter trained with DPO across multiple model sizes. We use the `ControlSpec` functionality to sweep over the number of few-shot examples in order to study how the DPO-LoRA control compares to the few-shot control as we scale the number of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb9d35",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pi8wn8f6sch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import PeftType\n",
    "\n",
    "from aisteer360.algorithms.input_control.few_shot.control import FewShot\n",
    "from aisteer360.algorithms.core.specs import ControlSpec\n",
    "from aisteer360.algorithms.structural_control.wrappers.trl.dpotrainer.control import DPO\n",
    "from aisteer360.evaluation.use_cases.commonsense_mcqa.use_case import CommonsenseMCQA\n",
    "from aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_accuracy import MCQAAccuracy\n",
    "from aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_positional_bias import MCQAPositionalBias\n",
    "from aisteer360.evaluation.benchmark import Benchmark\n",
    "from aisteer360.evaluation.utils import flatten_profiles, get_param_values, summarize_by_config\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "os.chdir(\"./examples/notebooks/benchmark_commonsense_mcqa/\")\n",
    "\n",
    "MODELS = [\n",
    "    # \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    # \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef1d6c-592b-47ae-85b5-962b46e1acc6",
   "metadata": {},
   "source": [
    "## Building the use case\n",
    "\n",
    "The use case of interest has already been constructed via the [use case](../../../docs/tutorials/add_new_use_case.md) tutorial and is available at `aisteer360/evaluation/use_cases/commonsense_mcqa/use_case.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff040222-ac29-4107-9bb5-22e66dde52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_mcqa = CommonsenseMCQA(\n",
    "    evaluation_data=\"evaluation_qa.jsonl\",\n",
    "    evaluation_metrics=[MCQAAccuracy(), MCQAPositionalBias()],\n",
    "    num_shuffling_runs=20,\n",
    "    num_samples=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e0f8a-caeb-4dcf-a07e-c7663a4dd4f0",
   "metadata": {},
   "source": [
    "Two custom metrics have been created for the use case: `MCQAAccuracy` which measures the accuracy statistics of each question (across trials), and `MCQAPositionalBias` which measures the positional bias (via deviation from the uniform distribution across runs). To facilitate computation of these statistics, the use case accepts a keyword argument `num_shuffling_runs` dictating how many times each question should be presented to the (steered) model under a randomized ordering of the choices. We restrict the number of evaluation datapoints to `num_samples=50` for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998bcf58-7635-4646-873b-ae38436b3f21",
   "metadata": {},
   "source": [
    "## Preparing the steering data\n",
    "\n",
    "The benchmark uses steering data consisting of triples `(question, answer_chosen, answer_rejected)` extracted from the CommonsenseQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8e2d5e-cdea-47aa-89e6-0985d2d0cc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4871,\n",
       " {'id': '01beaf20-82aa-40b0-8b08-ee08b94e6666',\n",
       "  'question': 'The spirit ascended to the after life, so what was it leaving?',\n",
       "  'answer_chosen': 'human being',\n",
       "  'answer_rejected': 'cemetary'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"steer_qa.jsonl\", \"r\") as f:\n",
    "    steering_data = [json.loads(line) for line in f]\n",
    "\n",
    "len(steering_data), steering_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db86c40-5e7a-4a2b-bfeb-0b9b82a983c9",
   "metadata": {},
   "source": [
    "For the `FewShot` control, we need to create example pools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b6a285-0719-47d1-a4fb-f5fa8526e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4871, 4871)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_pool = [{\"question\": row[\"question\"], \"answer\": row[\"answer_chosen\"]} for row in steering_data]\n",
    "negative_pool = [{\"question\": row[\"question\"], \"answer\": row[\"answer_rejected\"]} for row in steering_data]\n",
    "\n",
    "len(positive_pool), len(negative_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f80559-54d0-4740-a7c6-ca10f2d0c999",
   "metadata": {},
   "source": [
    "## Defining the controls\n",
    "\n",
    "### FewShot with ControlSpec\n",
    "\n",
    "Instead of using a fixed number of examples, we use `ControlSpec` to sweep over different values of `k_positive`. We fix `k_negative=0` to isolate the effect of positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd33d9d-30a7-4715-8af6-04080d8e87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_spec = ControlSpec(\n",
    "    control_cls=FewShot,\n",
    "    params={\n",
    "        \"selector_name\": \"random\",\n",
    "        \"positive_example_pool\": positive_pool,\n",
    "        \"negative_example_pool\": negative_pool,\n",
    "        \"k_negative\": 0,\n",
    "    },\n",
    "    vars=[{\"k_positive\": k} for k in [1, 5, 10, 25]],\n",
    "    name=\"FewShot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30194591-7892-496f-995e-59e3df656da1",
   "metadata": {},
   "source": [
    "### DPO with LoRA\n",
    "\n",
    "The DPO-LoRA control serves as our target to beat. It uses the same steering data to fine-tune a LoRA adapter. The hyperparameters below are tuned for the low-data regime (~5k examples): higher learning rate for faster convergence, lower beta for softer preference signals, and smaller LoRA rank to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959c7d46-ef89-4129-8f42-8449037a8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_list([\n",
    "    {\"prompt\": row[\"question\"], \"chosen\": row[\"answer_chosen\"], \"rejected\": row[\"answer_rejected\"]}\n",
    "    for row in steering_data\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dpo_factory_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dpo_control(model_name: str) -> DPO:\n",
    "    \"\"\"Create a DPO control with model-specific output directory.\"\"\"\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    return DPO(\n",
    "        train_dataset=train_ds,\n",
    "\n",
    "        # DPO / TRL config (tuned for low-data regime)\n",
    "        output_dir=f\"trl_models/{short_name}-DPO-Lora-Steer\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=5e-5,\n",
    "        beta=0.05,\n",
    "        loss_type=\"sigmoid\",\n",
    "        max_length=1024,\n",
    "        max_prompt_length=512,\n",
    "        disable_dropout=True,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        seed=123,\n",
    "\n",
    "        # LoRA config (smaller rank to reduce overfitting)\n",
    "        use_peft=True,\n",
    "        peft_type=PeftType.LORA,\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        adapter_name=\"dpo\",\n",
    "        merge_lora_after_train=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7ca68-0e1b-4fde-a235-b08c2537bd84",
   "metadata": {},
   "source": [
    "## Running the benchmark\n",
    "\n",
    "The benchmark compares three steering approaches across multiple model sizes:\n",
    "- **baseline**: Unsteered model\n",
    "- **few_shot_sweep**: FewShot with varying `k_positive` (1, 5, 10, 25)\n",
    "- **dpo_lora**: DPO-trained LoRA adapter\n",
    "\n",
    "We run with `num_trials=5` to capture statistical variability across generation runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0005b-c0b0-4879-acc3-241e3f014307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark for Qwen2.5-3B-Instruct\n",
      "Running pipeline: baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████| 2/2 [00:09<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Running pipeline: few_shot_sweep...\n",
      "Running configuration 1...\n",
      "Running configuration 2...\n"
     ]
    }
   ],
   "source": [
    "all_profiles = {}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"Running benchmark for {short_name}\")\n",
    "\n",
    "    dpo_lora = create_dpo_control(model_name)\n",
    "\n",
    "    benchmark = Benchmark(\n",
    "        use_case=commonsense_mcqa,\n",
    "        base_model_name_or_path=model_name,\n",
    "        steering_pipelines={\n",
    "            \"baseline\": [],\n",
    "            \"few_shot_sweep\": [few_shot_spec],\n",
    "            \"dpo_lora\": [dpo_lora],\n",
    "        },\n",
    "        gen_kwargs={\"max_new_tokens\": 300, \"do_sample\": True, \"temperature\": 0.7},\n",
    "        device_map=\"auto\",\n",
    "        num_trials=5\n",
    "    )\n",
    "\n",
    "    profiles = benchmark.run()\n",
    "    all_profiles[short_name] = profiles\n",
    "\n",
    "    # export per-model results\n",
    "    benchmark.export(profiles, save_dir=f\"./profiles/{short_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb6f43-ba41-4117-a366-6580013620bc",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We now analyze the benchmark results across all model sizes. With multiple trials, we can compute mean and standard deviation to understand the statistical reliability of our comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten_section",
   "metadata": {},
   "source": [
    "### Flatten and summarize results\n",
    "\n",
    "First, we flatten the nested profiles into a single DataFrame with one row per trial, then aggregate across trials to get mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86624b5e-6b67-413f-87b5-9acf241f39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten profiles from all models into a single DataFrame\n",
    "dfs = []\n",
    "for model_name, profiles in all_profiles.items():\n",
    "    df = flatten_profiles(\n",
    "        profiles,\n",
    "        metric_accessors={\n",
    "            \"accuracy\": (\"MCQAAccuracy\", \"question_mean\"),\n",
    "            \"positional_bias\": (\"MCQAPositionalBias\", \"mean\"),\n",
    "        }\n",
    "    )\n",
    "    df[\"model\"] = model_name\n",
    "    df[\"k_positive\"] = get_param_values(df, \"FewShot\", \"k_positive\")\n",
    "    dfs.append(df)\n",
    "\n",
    "runs_df = pd.concat(dfs, ignore_index=True)\n",
    "runs_df[[\"model\", \"pipeline\", \"trial_id\", \"k_positive\", \"accuracy\", \"positional_bias\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "summarize_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize by configuration (aggregate across trials)\n",
    "summary_df = summarize_by_config(\n",
    "    runs_df,\n",
    "    metric_cols=[\"accuracy\", \"positional_bias\"],\n",
    "    group_cols=[\"model\", \"pipeline\", \"config_id\"]\n",
    ")\n",
    "\n",
    "# add k_positive for few-shot rows\n",
    "k_map = runs_df.groupby([\"model\", \"pipeline\", \"config_id\"])[\"k_positive\"].first()\n",
    "summary_df[\"k_positive\"] = summary_df.apply(\n",
    "    lambda row: k_map.get((row[\"model\"], row[\"pipeline\"], row[\"config_id\"]), np.nan), axis=1\n",
    ")\n",
    "\n",
    "summary_df[[\"model\", \"pipeline\", \"k_positive\", \"n_trials\", \"accuracy_mean\", \"accuracy_std\", \"positional_bias_mean\", \"positional_bias_std\"]].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viz_section",
   "metadata": {},
   "source": [
    "### Baseline accuracy by model size\n",
    "\n",
    "We first examine how the unsteered baseline performance varies with model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_by_model_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = summary_df[summary_df[\"pipeline\"] == \"baseline\"].copy()\n",
    "baseline_df[\"model_size\"] = baseline_df[\"model\"].map({\n",
    "    \"Qwen2.5-0.5B-Instruct\": 0.5,\n",
    "    \"Qwen2.5-1.5B-Instruct\": 1.5,\n",
    "    \"Qwen2.5-3B-Instruct\": 3.0,\n",
    "})\n",
    "baseline_df = baseline_df.sort_values(\"model_size\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.bar(\n",
    "    baseline_df[\"model\"],\n",
    "    baseline_df[\"accuracy_mean\"],\n",
    "    yerr=baseline_df[\"accuracy_std\"],\n",
    "    capsize=5,\n",
    "    color=\"steelblue\",\n",
    "    edgecolor=\"black\"\n",
    ")\n",
    "ax.set_xlabel(\"model\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.set_title(\"baseline accuracy by model size\")\n",
    "ax.tick_params(axis=\"x\", rotation=15)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewshot_scaling_section",
   "metadata": {},
   "source": [
    "### FewShot scaling by model\n",
    "\n",
    "Next, we examine how FewShot accuracy scales with the number of positive examples across different model sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewshot_scaling_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_df = summary_df[summary_df[\"pipeline\"] == \"few_shot_sweep\"].copy()\n",
    "few_shot_df = few_shot_df.sort_values([\"model\", \"k_positive\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = {\"Qwen2.5-0.5B-Instruct\": \"C0\", \"Qwen2.5-1.5B-Instruct\": \"C1\", \"Qwen2.5-3B-Instruct\": \"C2\"}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    model_df = few_shot_df[few_shot_df[\"model\"] == short_name]\n",
    "    ax.errorbar(\n",
    "        model_df[\"k_positive\"],\n",
    "        model_df[\"accuracy_mean\"],\n",
    "        yerr=model_df[\"accuracy_std\"],\n",
    "        fmt=\"o-\",\n",
    "        capsize=4,\n",
    "        capthick=1.5,\n",
    "        markersize=8,\n",
    "        label=short_name,\n",
    "        color=colors[short_name]\n",
    "    )\n",
    "\n",
    "ax.set_xlabel(\"number of positive examples (k_positive)\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"FewShot accuracy scaling by model size\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross_method_section",
   "metadata": {},
   "source": [
    "### Cross-method comparison per model\n",
    "\n",
    "We compare baseline, best FewShot configuration, and DPO-LoRA for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross_method_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find best few-shot k for each model\n",
    "best_few_shot = few_shot_df.loc[few_shot_df.groupby(\"model\")[\"accuracy_mean\"].idxmax()].copy()\n",
    "best_few_shot[\"method\"] = best_few_shot[\"k_positive\"].apply(lambda k: f\"FewShot (k={int(k)})\")\n",
    "\n",
    "# get DPO results\n",
    "dpo_df = summary_df[summary_df[\"pipeline\"] == \"dpo_lora\"].copy()\n",
    "dpo_df[\"method\"] = \"DPO-LoRA\"\n",
    "\n",
    "# get baseline\n",
    "baseline_df_plot = baseline_df.copy()\n",
    "baseline_df_plot[\"method\"] = \"baseline\"\n",
    "\n",
    "# combine for plotting\n",
    "comparison_data = []\n",
    "for model_name in [m.split(\"/\")[-1] for m in MODELS]:\n",
    "    baseline_row = baseline_df_plot[baseline_df_plot[\"model\"] == model_name].iloc[0]\n",
    "    dpo_row = dpo_df[dpo_df[\"model\"] == model_name].iloc[0]\n",
    "    fs_row = best_few_shot[best_few_shot[\"model\"] == model_name].iloc[0]\n",
    "\n",
    "    comparison_data.append({\"model\": model_name, \"method\": \"baseline\", \"accuracy_mean\": baseline_row[\"accuracy_mean\"], \"accuracy_std\": baseline_row[\"accuracy_std\"]})\n",
    "    comparison_data.append({\"model\": model_name, \"method\": fs_row[\"method\"], \"accuracy_mean\": fs_row[\"accuracy_mean\"], \"accuracy_std\": fs_row[\"accuracy_std\"]})\n",
    "    comparison_data.append({\"model\": model_name, \"method\": \"DPO-LoRA\", \"accuracy_mean\": dpo_row[\"accuracy_mean\"], \"accuracy_std\": dpo_row[\"accuracy_std\"]})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grouped_bar_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "models = [m.split(\"/\")[-1] for m in MODELS]\n",
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "# group by method type\n",
    "baseline_vals = comparison_df[comparison_df[\"method\"] == \"baseline\"].set_index(\"model\").loc[models]\n",
    "dpo_vals = comparison_df[comparison_df[\"method\"] == \"DPO-LoRA\"].set_index(\"model\").loc[models]\n",
    "fs_vals = comparison_df[comparison_df[\"method\"].str.startswith(\"FewShot\")].set_index(\"model\").loc[models]\n",
    "\n",
    "ax.bar(x - width, baseline_vals[\"accuracy_mean\"], width, yerr=baseline_vals[\"accuracy_std\"], capsize=4, label=\"baseline\", color=\"gray\", edgecolor=\"black\")\n",
    "ax.bar(x, fs_vals[\"accuracy_mean\"], width, yerr=fs_vals[\"accuracy_std\"], capsize=4, label=\"best FewShot\", color=\"steelblue\", edgecolor=\"black\")\n",
    "ax.bar(x + width, dpo_vals[\"accuracy_mean\"], width, yerr=dpo_vals[\"accuracy_std\"], capsize=4, label=\"DPO-LoRA\", color=\"coral\", edgecolor=\"black\")\n",
    "\n",
    "ax.set_xlabel(\"model\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"accuracy comparison across steering methods\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(models, rotation=15)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.0)\n",
    "ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tradeoff_section",
   "metadata": {},
   "source": [
    "### Accuracy vs positional bias tradeoff\n",
    "\n",
    "We examine whether there is a tradeoff between accuracy and positional bias across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoff_scatter_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "for idx, model_name in enumerate([m.split(\"/\")[-1] for m in MODELS]):\n",
    "    ax = axes[idx]\n",
    "    model_summary = summary_df[summary_df[\"model\"] == model_name]\n",
    "\n",
    "    # baseline\n",
    "    bl = model_summary[model_summary[\"pipeline\"] == \"baseline\"]\n",
    "    ax.scatter(bl[\"accuracy_mean\"], bl[\"positional_bias_mean\"], marker=\"X\", s=150, c=\"red\", edgecolors=\"black\", zorder=4, label=\"baseline\")\n",
    "    ax.errorbar(bl[\"accuracy_mean\"], bl[\"positional_bias_mean\"], xerr=bl[\"accuracy_std\"], yerr=bl[\"positional_bias_std\"], fmt=\"none\", color=\"red\", alpha=0.5, capsize=2)\n",
    "\n",
    "    # DPO\n",
    "    dpo = model_summary[model_summary[\"pipeline\"] == \"dpo_lora\"]\n",
    "    ax.scatter(dpo[\"accuracy_mean\"], dpo[\"positional_bias_mean\"], marker=\"s\", s=120, c=\"coral\", edgecolors=\"black\", zorder=3, label=\"DPO-LoRA\")\n",
    "    ax.errorbar(dpo[\"accuracy_mean\"], dpo[\"positional_bias_mean\"], xerr=dpo[\"accuracy_std\"], yerr=dpo[\"positional_bias_std\"], fmt=\"none\", color=\"coral\", alpha=0.5, capsize=2)\n",
    "\n",
    "    # FewShot (color by k)\n",
    "    fs = model_summary[model_summary[\"pipeline\"] == \"few_shot_sweep\"].sort_values(\"k_positive\")\n",
    "    scatter = ax.scatter(fs[\"accuracy_mean\"], fs[\"positional_bias_mean\"], c=fs[\"k_positive\"], cmap=\"viridis\", s=100, edgecolors=\"black\", zorder=2, label=\"FewShot\")\n",
    "    for _, row in fs.iterrows():\n",
    "        ax.errorbar(row[\"accuracy_mean\"], row[\"positional_bias_mean\"], xerr=row[\"accuracy_std\"], yerr=row[\"positional_bias_std\"], fmt=\"none\", color=\"gray\", alpha=0.4, capsize=2)\n",
    "        ax.annotate(f\"k={int(row['k_positive'])}\", (row[\"accuracy_mean\"], row[\"positional_bias_mean\"]), textcoords=\"offset points\", xytext=(5, 5), fontsize=8)\n",
    "\n",
    "    ax.set_xlabel(\"accuracy\")\n",
    "    ax.set_ylabel(\"positional bias\")\n",
    "    ax.set_title(model_name)\n",
    "    ax.legend(loc=\"upper right\", fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"accuracy vs positional bias tradeoff\", y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "### Summary table\n",
    "\n",
    "The final summary table ranks all configurations by accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table = summary_df.copy()\n",
    "summary_table[\"method\"] = summary_table.apply(\n",
    "    lambda row: \"baseline\" if row[\"pipeline\"] == \"baseline\"\n",
    "    else \"DPO-LoRA\" if row[\"pipeline\"] == \"dpo_lora\"\n",
    "    else f\"FewShot (k={int(row['k_positive'])})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "display_df = summary_table[[\"model\", \"method\", \"n_trials\", \"accuracy_mean\", \"accuracy_std\", \"positional_bias_mean\", \"positional_bias_std\"]].copy()\n",
    "display_df.columns = [\"model\", \"method\", \"trials\", \"accuracy (mean)\", \"accuracy (std)\", \"pos bias (mean)\", \"pos bias (std)\"]\n",
    "display_df = display_df.sort_values([\"model\", \"accuracy (mean)\"], ascending=[True, False])\n",
    "\n",
    "display_df.style.format({\n",
    "    \"accuracy (mean)\": \"{:.1%}\",\n",
    "    \"accuracy (std)\": \"{:.1%}\",\n",
    "    \"pos bias (mean)\": \"{:.3f}\",\n",
    "    \"pos bias (std)\": \"{:.3f}\",\n",
    "}).background_gradient(subset=[\"accuracy (mean)\"], cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways_section",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

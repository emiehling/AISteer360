{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3290202-4183-4f46-81d3-a817d66d4f2b",
   "metadata": {},
   "source": [
    "# Commonsense MCQA\n",
    "\n",
    "Multiple choice question answering is a common format for evaluating a model's reasoning ability. This notebook benchmarks steering methods on the [CommonsenseQA](https://huggingface.co/datasets/tau/commonsense_qa) dataset, comparing few-shot prompting against a LoRA adapter trained with DPO. We sweep over the number of few-shot examples and study how accuracy scales relative to the fine-tuned baseline across a few different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c43de8",
   "metadata": {},
   "source": [
    "### Runtime Estimate\n",
    "\n",
    "> **Estimated Time:** 1 hour (approx. 20 minutes per each of the three models)  \n",
    "> **Device:** NVIDIA H100 GPU (80GB VRAM)\n",
    "\n",
    "Times are approximate and vary based on dataset size, number of sweeps, and model configuration. Adjust parameters in the cells below to modify runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb9d35",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pi8wn8f6sch",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from datasets import Dataset\n",
    "from peft import PeftType\n",
    "\n",
    "from aisteer360.algorithms.input_control.few_shot.control import FewShot\n",
    "from aisteer360.algorithms.core.specs import ControlSpec\n",
    "from aisteer360.algorithms.structural_control.wrappers.trl.dpotrainer.control import DPO\n",
    "from aisteer360.evaluation.use_cases.commonsense_mcqa.use_case import CommonsenseMCQA\n",
    "from aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_accuracy import MCQAAccuracy\n",
    "from aisteer360.evaluation.metrics.custom.commonsense_mcqa.mcqa_positional_bias import MCQAPositionalBias\n",
    "from aisteer360.evaluation.benchmark import Benchmark\n",
    "from aisteer360.evaluation.utils.data_utils import flatten_profiles, get_param_values, summarize_by_config\n",
    "from aisteer360.evaluation.utils.viz_utils import plot_sensitivity, plot_tradeoff\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "MODELS = [\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "]\n",
    "\n",
    "NOTEBOOK_DIR = Path(__file__).parent if \"__file__\" in dir() else Path.cwd() / \"examples/notebooks/benchmark_commonsense_mcqa\"\n",
    "FIGURE_DIR = NOTEBOOK_DIR / \"figures\"\n",
    "FIGURE_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef1d6c-592b-47ae-85b5-962b46e1acc6",
   "metadata": {},
   "source": [
    "## Building the use case\n",
    "\n",
    "The use case of interest has already been constructed via the [use case](../../../docs/tutorials/add_new_use_case.md) tutorial and is available at `aisteer360/evaluation/use_cases/commonsense_mcqa/use_case.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff040222-ac29-4107-9bb5-22e66dde52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "commonsense_mcqa = CommonsenseMCQA(\n",
    "    evaluation_data=NOTEBOOK_DIR / \"data/evaluation_qa.jsonl\",\n",
    "    evaluation_metrics=[MCQAAccuracy(), MCQAPositionalBias()],\n",
    "    num_shuffling_runs=20,\n",
    "    num_samples=50\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e0f8a-caeb-4dcf-a07e-c7663a4dd4f0",
   "metadata": {},
   "source": [
    "Two custom metrics have been created for the use case: `MCQAAccuracy` which measures the accuracy statistics of each question (across trials), and `MCQAPositionalBias` which measures the positional bias (via deviation from the uniform distribution across runs). To facilitate computation of these statistics, the use case accepts a keyword argument `num_shuffling_runs` dictating how many times each question should be presented to the (steered) model under a randomized ordering of the choices. We restrict the number of evaluation datapoints to `num_samples=50` for speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998bcf58-7635-4646-873b-ae38436b3f21",
   "metadata": {},
   "source": [
    "## Preparing the steering data\n",
    "\n",
    "The benchmark uses steering data consisting of triples `(question, answer_chosen, answer_rejected)` extracted from the CommonsenseQA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8e2d5e-cdea-47aa-89e6-0985d2d0cc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4871,\n",
       " {'id': '01beaf20-82aa-40b0-8b08-ee08b94e6666',\n",
       "  'question': 'The spirit ascended to the after life, so what was it leaving?',\n",
       "  'answer_chosen': 'human being',\n",
       "  'answer_rejected': 'cemetary'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(NOTEBOOK_DIR / \"data/steer_qa.jsonl\", \"r\") as f:\n",
    "    steering_data = [json.loads(line) for line in f]\n",
    "\n",
    "len(steering_data), steering_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db86c40-5e7a-4a2b-bfeb-0b9b82a983c9",
   "metadata": {},
   "source": [
    "For the `FewShot` control, we need to create example pools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b6a285-0719-47d1-a4fb-f5fa8526e8ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4871, 4871)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_pool = [{\"question\": row[\"question\"], \"answer\": row[\"answer_chosen\"]} for row in steering_data]\n",
    "negative_pool = [{\"question\": row[\"question\"], \"answer\": row[\"answer_rejected\"]} for row in steering_data]\n",
    "\n",
    "len(positive_pool), len(negative_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f80559-54d0-4740-a7c6-ca10f2d0c999",
   "metadata": {},
   "source": [
    "## Defining the controls\n",
    "\n",
    "### FewShot with ControlSpec\n",
    "\n",
    "Instead of using a fixed number of examples, we use `ControlSpec` to sweep over different values of `k_positive`. We fix `k_negative=0` to isolate the effect of positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cd33d9d-30a7-4715-8af6-04080d8e87b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_spec = ControlSpec(\n",
    "    control_cls=FewShot,\n",
    "    params={\n",
    "        \"selector_name\": \"random\",\n",
    "        \"positive_example_pool\": positive_pool,\n",
    "        \"negative_example_pool\": negative_pool,\n",
    "        \"k_negative\": 0,\n",
    "    },\n",
    "    vars=[{\"k_positive\": k} for k in [1, 5, 10, 25]],\n",
    "    name=\"FewShot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30194591-7892-496f-995e-59e3df656da1",
   "metadata": {},
   "source": [
    "### DPO with LoRA\n",
    "\n",
    "The DPO-LoRA control serves as our target to beat. It uses the same steering data to fine-tune a LoRA adapter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "959c7d46-ef89-4129-8f42-8449037a8c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = Dataset.from_list([\n",
    "    {\"prompt\": row[\"question\"], \"chosen\": row[\"answer_chosen\"], \"rejected\": row[\"answer_rejected\"]}\n",
    "    for row in steering_data\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23fa4388",
   "metadata": {},
   "source": [
    "Since we are comparing the behavior of multiple models (of different sizes) in this notebook, it is generally advised to use different training parameters. Smaller models use higher learning rates and more epochs for faster convergence, while larger models use more conservative settings (lower learning rate, higher beta, larger LoRA rank) to avoid overfitting on the relatively small training set (~5k examples). We encode these differences in `DPO_CONFIGS` and create a simple function wrapper function (`create_dpo_control`) to instantiate the respective controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dpo_factory_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model-specific DPO hyperparameters\n",
    "DPO_CONFIGS = {\n",
    "    \"Qwen/Qwen2.5-0.5B-Instruct\": {\n",
    "        \"learning_rate\": 5e-5,\n",
    "        \"beta\": 0.05,\n",
    "        \"num_train_epochs\": 3,\n",
    "        \"r\": 8,\n",
    "        \"lora_alpha\": 16,\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\": {\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"beta\": 0.1,\n",
    "        \"num_train_epochs\": 2,\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "    },\n",
    "    \"Qwen/Qwen2.5-3B-Instruct\": {\n",
    "        \"learning_rate\": 1e-5,\n",
    "        \"beta\": 0.1,\n",
    "        \"num_train_epochs\": 2,\n",
    "        \"r\": 16,\n",
    "        \"lora_alpha\": 32,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def create_dpo_control(model_name: str) -> DPO:\n",
    "    \"\"\"Create a DPO control with model-specific hyperparameters.\"\"\"\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    config = DPO_CONFIGS.get(model_name, DPO_CONFIGS[\"Qwen/Qwen2.5-0.5B-Instruct\"])\n",
    "\n",
    "    return DPO(\n",
    "        train_dataset=train_ds,\n",
    "\n",
    "        # DPO / TRL config (model-specific)\n",
    "        output_dir=NOTEBOOK_DIR / f\"trl_models/{short_name}-DPO-Lora-Steer\",\n",
    "        per_device_train_batch_size=4,\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        beta=config[\"beta\"],\n",
    "        loss_type=\"sigmoid\",\n",
    "        max_length=1024,\n",
    "        max_prompt_length=512,\n",
    "        disable_dropout=True,\n",
    "        logging_steps=100,\n",
    "        save_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        seed=123,\n",
    "\n",
    "        # LoRA config (model-specific)\n",
    "        use_peft=True,\n",
    "        peft_type=PeftType.LORA,\n",
    "        r=config[\"r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        adapter_name=\"dpo\",\n",
    "        merge_lora_after_train=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a7ca68-0e1b-4fde-a235-b08c2537bd84",
   "metadata": {},
   "source": [
    "## Running the benchmark\n",
    "\n",
    "The benchmark compares three steering approaches across multiple model sizes:\n",
    "- **baseline**: Unsteered model\n",
    "- **few_shot_sweep**: FewShot with varying `k_positive` (1, 5, 10, 25)\n",
    "- **dpo_lora**: DPO-trained LoRA adapter\n",
    "\n",
    "We run with `num_trials=5` to capture statistical variability across generation runs (at the cost of slower execution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb0005b-c0b0-4879-acc3-241e3f014307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running benchmark for Qwen2.5-0.5B-Instruct\n",
      "Running pipeline: baseline...\n",
      "done.\n",
      "Running pipeline: few_shot_sweep...\n",
      "Running configuration 1...\n",
      "Running configuration 2...\n",
      "Running configuration 3...\n",
      "Running configuration 4...\n",
      "done.\n",
      "Running pipeline: dpo_lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 50578.07 examples/s]\n",
      "Extracting prompt in train dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 33117.88 examples/s]\n",
      "Applying chat template to train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 39726.63 examples/s]\n",
      "Tokenizing train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 6486.53 examples/s]\n",
      "Train dataset reference log probs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1218/1218 [05:09<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6888, 'grad_norm': 1.1701231002807617, 'learning_rate': 4.8645320197044334e-05, 'rewards/chosen': 0.03167436644434929, 'rewards/rejected': 0.02267163060605526, 'rewards/accuracies': 0.6025000214576721, 'rewards/margins': 0.009002731181681156, 'logps/chosen': -40.01519775390625, 'logps/rejected': -42.47361373901367, 'logits/chosen': -1.2978674173355103, 'logits/rejected': -1.2629873752593994, 'epoch': 0.08210180623973727}\n",
      "{'loss': 0.6657, 'grad_norm': 1.5464173555374146, 'learning_rate': 4.7276956759715385e-05, 'rewards/chosen': 0.19636569917201996, 'rewards/rejected': 0.13458965718746185, 'rewards/accuracies': 0.6875, 'rewards/margins': 0.061776045709848404, 'logps/chosen': -36.535667419433594, 'logps/rejected': -39.877471923828125, 'logits/chosen': -1.5315977334976196, 'logits/rejected': -1.519681692123413, 'epoch': 0.16420361247947454}\n",
      "{'loss': 0.6149, 'grad_norm': 4.150752067565918, 'learning_rate': 4.590859332238643e-05, 'rewards/chosen': 0.41691139340400696, 'rewards/rejected': 0.23136243224143982, 'rewards/accuracies': 0.7524999976158142, 'rewards/margins': 0.18554897606372833, 'logps/chosen': -32.674896240234375, 'logps/rejected': -38.3759880065918, 'logits/chosen': -2.3622031211853027, 'logits/rejected': -2.3564064502716064, 'epoch': 0.24630541871921183}\n",
      "{'loss': 0.6, 'grad_norm': 3.890317440032959, 'learning_rate': 4.454022988505747e-05, 'rewards/chosen': 0.5023082494735718, 'rewards/rejected': 0.2609112858772278, 'rewards/accuracies': 0.7350000143051147, 'rewards/margins': 0.24139702320098877, 'logps/chosen': -30.91019058227539, 'logps/rejected': -37.9001579284668, 'logits/chosen': -2.730015277862549, 'logits/rejected': -2.712534189224243, 'epoch': 0.3284072249589491}\n",
      "{'loss': 0.5745, 'grad_norm': 5.194489479064941, 'learning_rate': 4.3171866447728524e-05, 'rewards/chosen': 0.5490263104438782, 'rewards/rejected': 0.23788800835609436, 'rewards/accuracies': 0.7475000023841858, 'rewards/margins': 0.3111383020877838, 'logps/chosen': -30.26494598388672, 'logps/rejected': -38.37617111206055, 'logits/chosen': -2.6364450454711914, 'logits/rejected': -2.609036922454834, 'epoch': 0.41050903119868637}\n",
      "{'loss': 0.5381, 'grad_norm': 4.84236478805542, 'learning_rate': 4.180350301039956e-05, 'rewards/chosen': 0.597510576248169, 'rewards/rejected': 0.17771625518798828, 'rewards/accuracies': 0.7599999904632568, 'rewards/margins': 0.41979432106018066, 'logps/chosen': -28.864492416381836, 'logps/rejected': -39.21742630004883, 'logits/chosen': -2.6501526832580566, 'logits/rejected': -2.6283562183380127, 'epoch': 0.49261083743842365}\n",
      "{'loss': 0.5149, 'grad_norm': 3.8041231632232666, 'learning_rate': 4.0435139573070605e-05, 'rewards/chosen': 0.6611571311950684, 'rewards/rejected': 0.1473751664161682, 'rewards/accuracies': 0.800000011920929, 'rewards/margins': 0.5137819051742554, 'logps/chosen': -27.251386642456055, 'logps/rejected': -40.09393310546875, 'logits/chosen': -2.4396955966949463, 'logits/rejected': -2.382899284362793, 'epoch': 0.5747126436781609}\n",
      "{'loss': 0.5448, 'grad_norm': 4.921773910522461, 'learning_rate': 3.9066776135741656e-05, 'rewards/chosen': 0.6853637099266052, 'rewards/rejected': 0.23677606880664825, 'rewards/accuracies': 0.737500011920929, 'rewards/margins': 0.44858765602111816, 'logps/chosen': -27.14435577392578, 'logps/rejected': -37.99489212036133, 'logits/chosen': -2.383570432662964, 'logits/rejected': -2.378180742263794, 'epoch': 0.6568144499178982}\n",
      "{'loss': 0.5433, 'grad_norm': 5.648695468902588, 'learning_rate': 3.76984126984127e-05, 'rewards/chosen': 0.6686890125274658, 'rewards/rejected': 0.20555497705936432, 'rewards/accuracies': 0.7599999904632568, 'rewards/margins': 0.4631340801715851, 'logps/chosen': -27.064733505249023, 'logps/rejected': -38.92092514038086, 'logits/chosen': -2.3125882148742676, 'logits/rejected': -2.293424129486084, 'epoch': 0.7389162561576355}\n",
      "{'loss': 0.4828, 'grad_norm': 8.616083145141602, 'learning_rate': 3.6330049261083744e-05, 'rewards/chosen': 0.6742419600486755, 'rewards/rejected': 0.022235220298171043, 'rewards/accuracies': 0.800000011920929, 'rewards/margins': 0.6520067453384399, 'logps/chosen': -27.322372436523438, 'logps/rejected': -42.3288459777832, 'logits/chosen': -2.1432180404663086, 'logits/rejected': -2.186343193054199, 'epoch': 0.8210180623973727}\n",
      "{'loss': 0.4977, 'grad_norm': 9.864547729492188, 'learning_rate': 3.4961685823754795e-05, 'rewards/chosen': 0.7058195471763611, 'rewards/rejected': 0.11272397637367249, 'rewards/accuracies': 0.7825000286102295, 'rewards/margins': 0.5930955410003662, 'logps/chosen': -26.514320373535156, 'logps/rejected': -40.680171966552734, 'logits/chosen': -2.1039376258850098, 'logits/rejected': -2.1263251304626465, 'epoch': 0.90311986863711}\n",
      "{'loss': 0.5234, 'grad_norm': 6.591056823730469, 'learning_rate': 3.359332238642583e-05, 'rewards/chosen': 0.7249676585197449, 'rewards/rejected': 0.1610993891954422, 'rewards/accuracies': 0.7524999976158142, 'rewards/margins': 0.5638682842254639, 'logps/chosen': -26.353899002075195, 'logps/rejected': -39.85254669189453, 'logits/chosen': -2.109844923019409, 'logits/rejected': -2.1053977012634277, 'epoch': 0.9852216748768473}\n",
      "{'loss': 0.4666, 'grad_norm': 10.462512969970703, 'learning_rate': 3.222495894909688e-05, 'rewards/chosen': 0.7756478786468506, 'rewards/rejected': 0.05454007163643837, 'rewards/accuracies': 0.824999988079071, 'rewards/margins': 0.7211077809333801, 'logps/chosen': -25.365053176879883, 'logps/rejected': -41.94956970214844, 'logits/chosen': -2.1065378189086914, 'logits/rejected': -2.139610528945923, 'epoch': 1.0673234811165846}\n",
      "{'loss': 0.4435, 'grad_norm': 5.403989315032959, 'learning_rate': 3.085659551176793e-05, 'rewards/chosen': 0.7883327603340149, 'rewards/rejected': -0.008924959227442741, 'rewards/accuracies': 0.8324999809265137, 'rewards/margins': 0.797257661819458, 'logps/chosen': -24.89864730834961, 'logps/rejected': -42.77696228027344, 'logits/chosen': -2.0398106575012207, 'logits/rejected': -2.096579074859619, 'epoch': 1.1494252873563218}\n",
      "{'loss': 0.4681, 'grad_norm': 10.781396865844727, 'learning_rate': 2.948823207443897e-05, 'rewards/chosen': 0.7173644304275513, 'rewards/rejected': -0.037970758974552155, 'rewards/accuracies': 0.7850000262260437, 'rewards/margins': 0.7553351521492004, 'logps/chosen': -26.386878967285156, 'logps/rejected': -43.817569732666016, 'logits/chosen': -2.1184885501861572, 'logits/rejected': -2.1504697799682617, 'epoch': 1.2315270935960592}\n",
      "{'loss': 0.4551, 'grad_norm': 8.992352485656738, 'learning_rate': 2.8119868637110018e-05, 'rewards/chosen': 0.6872003674507141, 'rewards/rejected': -0.1227094754576683, 'rewards/accuracies': 0.7975000143051147, 'rewards/margins': 0.8099098205566406, 'logps/chosen': -26.953325271606445, 'logps/rejected': -45.0635986328125, 'logits/chosen': -2.1082985401153564, 'logits/rejected': -2.1482903957366943, 'epoch': 1.3136288998357963}\n",
      "{'loss': 0.4643, 'grad_norm': 19.6455078125, 'learning_rate': 2.6751505199781062e-05, 'rewards/chosen': 0.6228633522987366, 'rewards/rejected': -0.14114700257778168, 'rewards/accuracies': 0.8149999976158142, 'rewards/margins': 0.7640103101730347, 'logps/chosen': -28.22663116455078, 'logps/rejected': -45.91587829589844, 'logits/chosen': -2.050624132156372, 'logits/rejected': -2.1371395587921143, 'epoch': 1.3957307060755337}\n",
      "{'loss': 0.4359, 'grad_norm': 9.076135635375977, 'learning_rate': 2.5383141762452106e-05, 'rewards/chosen': 0.7654681205749512, 'rewards/rejected': -0.060600005090236664, 'rewards/accuracies': 0.8349999785423279, 'rewards/margins': 0.8260681629180908, 'logps/chosen': -25.657114028930664, 'logps/rejected': -44.17072677612305, 'logits/chosen': -1.9971832036972046, 'logits/rejected': -1.9909404516220093, 'epoch': 1.477832512315271}\n",
      "{'loss': 0.4132, 'grad_norm': 14.512584686279297, 'learning_rate': 2.4014778325123154e-05, 'rewards/chosen': 0.7597508430480957, 'rewards/rejected': -0.17522472143173218, 'rewards/accuracies': 0.824999988079071, 'rewards/margins': 0.9349755048751831, 'logps/chosen': -25.46965789794922, 'logps/rejected': -46.11054611206055, 'logits/chosen': -2.022824764251709, 'logits/rejected': -2.0512752532958984, 'epoch': 1.5599343185550083}\n",
      "{'loss': 0.4043, 'grad_norm': 31.62288475036621, 'learning_rate': 2.2646414887794198e-05, 'rewards/chosen': 0.5946488976478577, 'rewards/rejected': -0.4056760370731354, 'rewards/accuracies': 0.8299999833106995, 'rewards/margins': 1.0003249645233154, 'logps/chosen': -28.797748565673828, 'logps/rejected': -51.185089111328125, 'logits/chosen': -1.9222239255905151, 'logits/rejected': -1.9532742500305176, 'epoch': 1.6420361247947455}\n",
      "{'loss': 0.4117, 'grad_norm': 17.65192222595215, 'learning_rate': 2.1278051450465245e-05, 'rewards/chosen': 0.5898006558418274, 'rewards/rejected': -0.4164646565914154, 'rewards/accuracies': 0.8274999856948853, 'rewards/margins': 1.0062651634216309, 'logps/chosen': -29.13323402404785, 'logps/rejected': -51.79323196411133, 'logits/chosen': -1.9504092931747437, 'logits/rejected': -2.010681390762329, 'epoch': 1.7241379310344827}\n",
      "{'loss': 0.4289, 'grad_norm': 4.0471415519714355, 'learning_rate': 1.990968801313629e-05, 'rewards/chosen': 0.623900830745697, 'rewards/rejected': -0.34680992364883423, 'rewards/accuracies': 0.8149999976158142, 'rewards/margins': 0.9707107543945312, 'logps/chosen': -28.344337463378906, 'logps/rejected': -49.9163818359375, 'logits/chosen': -1.9579782485961914, 'logits/rejected': -1.9726163148880005, 'epoch': 1.80623973727422}\n",
      "{'loss': 0.4507, 'grad_norm': 11.161017417907715, 'learning_rate': 1.8541324575807333e-05, 'rewards/chosen': 0.6618115901947021, 'rewards/rejected': -0.22646695375442505, 'rewards/accuracies': 0.7825000286102295, 'rewards/margins': 0.888278603553772, 'logps/chosen': -27.318614959716797, 'logps/rejected': -47.125736236572266, 'logits/chosen': -1.8562203645706177, 'logits/rejected': -1.8680915832519531, 'epoch': 1.8883415435139574}\n",
      "{'loss': 0.4596, 'grad_norm': 5.649340629577637, 'learning_rate': 1.717296113847838e-05, 'rewards/chosen': 0.5495545864105225, 'rewards/rejected': -0.34036269783973694, 'rewards/accuracies': 0.8125, 'rewards/margins': 0.8899171352386475, 'logps/chosen': -29.875255584716797, 'logps/rejected': -49.97306823730469, 'logits/chosen': -1.9345932006835938, 'logits/rejected': -2.000433921813965, 'epoch': 1.9704433497536946}\n",
      "{'loss': 0.4039, 'grad_norm': 11.551080703735352, 'learning_rate': 1.5804597701149425e-05, 'rewards/chosen': 0.6284496784210205, 'rewards/rejected': -0.4037221670150757, 'rewards/accuracies': 0.8424999713897705, 'rewards/margins': 1.0321717262268066, 'logps/chosen': -28.410547256469727, 'logps/rejected': -50.81645202636719, 'logits/chosen': -1.817265510559082, 'logits/rejected': -1.9040954113006592, 'epoch': 2.052545155993432}\n",
      "{'loss': 0.3823, 'grad_norm': 6.470544815063477, 'learning_rate': 1.4436234263820472e-05, 'rewards/chosen': 0.6767504215240479, 'rewards/rejected': -0.4233320355415344, 'rewards/accuracies': 0.8399999737739563, 'rewards/margins': 1.100082516670227, 'logps/chosen': -27.319604873657227, 'logps/rejected': -51.23472213745117, 'logits/chosen': -1.8125343322753906, 'logits/rejected': -1.8773682117462158, 'epoch': 2.134646962233169}\n",
      "{'loss': 0.4038, 'grad_norm': 13.676458358764648, 'learning_rate': 1.3067870826491516e-05, 'rewards/chosen': 0.6101954579353333, 'rewards/rejected': -0.4773973524570465, 'rewards/accuracies': 0.8199999928474426, 'rewards/margins': 1.0875927209854126, 'logps/chosen': -28.629467010498047, 'logps/rejected': -52.20183563232422, 'logits/chosen': -1.7851513624191284, 'logits/rejected': -1.8641655445098877, 'epoch': 2.2167487684729066}\n",
      "{'loss': 0.366, 'grad_norm': 18.590909957885742, 'learning_rate': 1.1699507389162562e-05, 'rewards/chosen': 0.6153707504272461, 'rewards/rejected': -0.5237850546836853, 'rewards/accuracies': 0.8675000071525574, 'rewards/margins': 1.1391558647155762, 'logps/chosen': -28.71529769897461, 'logps/rejected': -53.10749053955078, 'logits/chosen': -1.8338360786437988, 'logits/rejected': -1.8971469402313232, 'epoch': 2.2988505747126435}\n",
      "{'loss': 0.3861, 'grad_norm': 13.608030319213867, 'learning_rate': 1.0331143951833607e-05, 'rewards/chosen': 0.5734960436820984, 'rewards/rejected': -0.6081339716911316, 'rewards/accuracies': 0.8475000262260437, 'rewards/margins': 1.1816298961639404, 'logps/chosen': -29.138290405273438, 'logps/rejected': -55.391719818115234, 'logits/chosen': -1.8384028673171997, 'logits/rejected': -1.9166373014450073, 'epoch': 2.380952380952381}\n",
      "{'loss': 0.3772, 'grad_norm': 9.13829517364502, 'learning_rate': 8.962780514504653e-06, 'rewards/chosen': 0.5543387532234192, 'rewards/rejected': -0.6267732381820679, 'rewards/accuracies': 0.8450000286102295, 'rewards/margins': 1.1811119318008423, 'logps/chosen': -29.539365768432617, 'logps/rejected': -55.82963943481445, 'logits/chosen': -1.7857133150100708, 'logits/rejected': -1.8092145919799805, 'epoch': 2.4630541871921183}\n",
      "{'loss': 0.4171, 'grad_norm': 5.094829559326172, 'learning_rate': 7.594417077175699e-06, 'rewards/chosen': 0.5165606141090393, 'rewards/rejected': -0.5711789131164551, 'rewards/accuracies': 0.8199999928474426, 'rewards/margins': 1.0877395868301392, 'logps/chosen': -30.712970733642578, 'logps/rejected': -54.20936965942383, 'logits/chosen': -1.7253615856170654, 'logits/rejected': -1.7834659814834595, 'epoch': 2.5451559934318553}\n",
      "{'loss': 0.3951, 'grad_norm': 16.369611740112305, 'learning_rate': 6.226053639846744e-06, 'rewards/chosen': 0.5839089751243591, 'rewards/rejected': -0.5823429226875305, 'rewards/accuracies': 0.8374999761581421, 'rewards/margins': 1.1662518978118896, 'logps/chosen': -29.054584503173828, 'logps/rejected': -54.532310485839844, 'logits/chosen': -1.7187647819519043, 'logits/rejected': -1.7938576936721802, 'epoch': 2.6272577996715927}\n",
      "{'loss': 0.4318, 'grad_norm': 35.13059997558594, 'learning_rate': 4.857690202517789e-06, 'rewards/chosen': 0.4108965992927551, 'rewards/rejected': -0.6428661942481995, 'rewards/accuracies': 0.8075000047683716, 'rewards/margins': 1.0537627935409546, 'logps/chosen': -32.720176696777344, 'logps/rejected': -56.04270935058594, 'logits/chosen': -1.738551139831543, 'logits/rejected': -1.8497298955917358, 'epoch': 2.70935960591133}\n",
      "{'loss': 0.3846, 'grad_norm': 12.162073135375977, 'learning_rate': 3.4893267651888343e-06, 'rewards/chosen': 0.5831906199455261, 'rewards/rejected': -0.6113725900650024, 'rewards/accuracies': 0.8424999713897705, 'rewards/margins': 1.1945632696151733, 'logps/chosen': -29.180715560913086, 'logps/rejected': -55.073402404785156, 'logits/chosen': -1.7367644309997559, 'logits/rejected': -1.8284804821014404, 'epoch': 2.7914614121510675}\n",
      "{'loss': 0.3813, 'grad_norm': 13.318687438964844, 'learning_rate': 2.1209633278598796e-06, 'rewards/chosen': 0.49279555678367615, 'rewards/rejected': -0.7236541509628296, 'rewards/accuracies': 0.8424999713897705, 'rewards/margins': 1.2164496183395386, 'logps/chosen': -30.520071029663086, 'logps/rejected': -57.61033630371094, 'logits/chosen': -1.7402145862579346, 'logits/rejected': -1.8035895824432373, 'epoch': 2.873563218390805}\n",
      "{'loss': 0.3742, 'grad_norm': 11.17454719543457, 'learning_rate': 7.52599890530925e-07, 'rewards/chosen': 0.5545939803123474, 'rewards/rejected': -0.676369845867157, 'rewards/accuracies': 0.8450000286102295, 'rewards/margins': 1.230963945388794, 'logps/chosen': -29.56679916381836, 'logps/rejected': -56.654354095458984, 'logits/chosen': -1.6961169242858887, 'logits/rejected': -1.8164033889770508, 'epoch': 2.955665024630542}\n",
      "{'train_runtime': 227.5256, 'train_samples_per_second': 64.226, 'train_steps_per_second': 16.06, 'train_loss': 0.4656423601088918, 'epoch': 3.0}\n",
      "done.\n",
      "Running benchmark for Qwen2.5-1.5B-Instruct\n",
      "Running pipeline: baseline...\n",
      "done.\n",
      "Running pipeline: few_shot_sweep...\n",
      "Running configuration 1...\n",
      "Running configuration 2...\n",
      "Running configuration 3...\n",
      "Running configuration 4...\n",
      "done.\n",
      "Running pipeline: dpo_lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 52394.78 examples/s]\n",
      "Extracting prompt in train dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 33447.53 examples/s]\n",
      "Applying chat template to train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 39230.94 examples/s]\n",
      "Tokenizing train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 6544.56 examples/s]\n",
      "Train dataset reference log probs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1218/1218 [05:28<00:00,  3.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6799, 'grad_norm': 0.747081458568573, 'learning_rate': 4.8645320197044334e-05, 'rewards/chosen': 0.0847334936261177, 'rewards/rejected': 0.05650382116436958, 'rewards/accuracies': 0.6924999952316284, 'rewards/margins': 0.028229672461748123, 'logps/chosen': -41.07448196411133, 'logps/rejected': -43.822601318359375, 'logits/chosen': 0.8180859088897705, 'logits/rejected': 0.8961646556854248, 'epoch': 0.08210180623973727}\n",
      "{'loss': 0.6152, 'grad_norm': 1.4636225700378418, 'learning_rate': 4.7276956759715385e-05, 'rewards/chosen': 0.31208381056785583, 'rewards/rejected': 0.12476341426372528, 'rewards/accuracies': 0.7699999809265137, 'rewards/margins': 0.18732041120529175, 'logps/chosen': -36.116390228271484, 'logps/rejected': -43.04884719848633, 'logits/chosen': 0.10593454539775848, 'logits/rejected': 0.21649768948554993, 'epoch': 0.16420361247947454}\n",
      "{'loss': 0.5299, 'grad_norm': 3.6608152389526367, 'learning_rate': 4.590859332238643e-05, 'rewards/chosen': 0.4018767476081848, 'rewards/rejected': -0.034979917109012604, 'rewards/accuracies': 0.8050000071525574, 'rewards/margins': 0.4368566870689392, 'logps/chosen': -34.749412536621094, 'logps/rejected': -46.063480377197266, 'logits/chosen': -1.834486961364746, 'logits/rejected': -1.8276875019073486, 'epoch': 0.24630541871921183}\n",
      "{'loss': 0.4649, 'grad_norm': 2.450493812561035, 'learning_rate': 4.454022988505747e-05, 'rewards/chosen': 0.5119261741638184, 'rewards/rejected': -0.1238023191690445, 'rewards/accuracies': 0.8525000214576721, 'rewards/margins': 0.6357285380363464, 'logps/chosen': -32.83365249633789, 'logps/rejected': -47.81764602661133, 'logits/chosen': -2.310502290725708, 'logits/rejected': -2.356104850769043, 'epoch': 0.3284072249589491}\n",
      "{'loss': 0.4453, 'grad_norm': 5.493669509887695, 'learning_rate': 4.3171866447728524e-05, 'rewards/chosen': 0.583362877368927, 'rewards/rejected': -0.20778389275074005, 'rewards/accuracies': 0.8349999785423279, 'rewards/margins': 0.7911467552185059, 'logps/chosen': -31.90009117126465, 'logps/rejected': -49.7025032043457, 'logits/chosen': -1.945203423500061, 'logits/rejected': -1.9796162843704224, 'epoch': 0.41050903119868637}\n",
      "{'loss': 0.4183, 'grad_norm': 5.644082546234131, 'learning_rate': 4.180350301039956e-05, 'rewards/chosen': 0.5615108609199524, 'rewards/rejected': -0.4265446364879608, 'rewards/accuracies': 0.8299999833106995, 'rewards/margins': 0.9880556464195251, 'logps/chosen': -31.480207443237305, 'logps/rejected': -53.678340911865234, 'logits/chosen': -1.549241065979004, 'logits/rejected': -1.6721196174621582, 'epoch': 0.49261083743842365}\n",
      "{'loss': 0.3641, 'grad_norm': 7.024675369262695, 'learning_rate': 4.0435139573070605e-05, 'rewards/chosen': 0.498069167137146, 'rewards/rejected': -0.7387111783027649, 'rewards/accuracies': 0.875, 'rewards/margins': 1.2367804050445557, 'logps/chosen': -32.40221405029297, 'logps/rejected': -60.29736328125, 'logits/chosen': -1.105889081954956, 'logits/rejected': -1.1922941207885742, 'epoch': 0.5747126436781609}\n",
      "{'loss': 0.3747, 'grad_norm': 5.127680778503418, 'learning_rate': 3.9066776135741656e-05, 'rewards/chosen': 0.45916247367858887, 'rewards/rejected': -0.8076040148735046, 'rewards/accuracies': 0.8349999785423279, 'rewards/margins': 1.2667665481567383, 'logps/chosen': -33.71772766113281, 'logps/rejected': -61.1474494934082, 'logits/chosen': -1.2902424335479736, 'logits/rejected': -1.3376493453979492, 'epoch': 0.6568144499178982}\n",
      "{'loss': 0.362, 'grad_norm': 14.431838989257812, 'learning_rate': 3.76984126984127e-05, 'rewards/chosen': 0.4920201599597931, 'rewards/rejected': -0.9333492517471313, 'rewards/accuracies': 0.8500000238418579, 'rewards/margins': 1.4253692626953125, 'logps/chosen': -32.53324890136719, 'logps/rejected': -63.67143249511719, 'logits/chosen': -1.483302354812622, 'logits/rejected': -1.5369640588760376, 'epoch': 0.7389162561576355}\n",
      "{'loss': 0.3396, 'grad_norm': 13.405555725097656, 'learning_rate': 3.6330049261083744e-05, 'rewards/chosen': 0.4881162941455841, 'rewards/rejected': -1.0439155101776123, 'rewards/accuracies': 0.8650000095367432, 'rewards/margins': 1.5320316553115845, 'logps/chosen': -32.622581481933594, 'logps/rejected': -65.93253326416016, 'logits/chosen': -1.6380746364593506, 'logits/rejected': -1.6653558015823364, 'epoch': 0.8210180623973727}\n",
      "{'loss': 0.3254, 'grad_norm': 9.244542121887207, 'learning_rate': 3.4961685823754795e-05, 'rewards/chosen': 0.49080196022987366, 'rewards/rejected': -1.1529022455215454, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 1.6437041759490967, 'logps/chosen': -32.98503494262695, 'logps/rejected': -67.98934936523438, 'logits/chosen': -1.5722830295562744, 'logits/rejected': -1.680911898612976, 'epoch': 0.90311986863711}\n",
      "{'loss': 0.3652, 'grad_norm': 7.192566394805908, 'learning_rate': 3.359332238642583e-05, 'rewards/chosen': 0.4608083665370941, 'rewards/rejected': -1.1794312000274658, 'rewards/accuracies': 0.8374999761581421, 'rewards/margins': 1.6402392387390137, 'logps/chosen': -33.88032150268555, 'logps/rejected': -69.00896453857422, 'logits/chosen': -1.4822332859039307, 'logits/rejected': -1.5314240455627441, 'epoch': 0.9852216748768473}\n",
      "{'loss': 0.3313, 'grad_norm': 4.000753879547119, 'learning_rate': 3.222495894909688e-05, 'rewards/chosen': 0.6010763049125671, 'rewards/rejected': -1.2741814851760864, 'rewards/accuracies': 0.8450000286102295, 'rewards/margins': 1.8752577304840088, 'logps/chosen': -30.73178482055664, 'logps/rejected': -70.94866943359375, 'logits/chosen': -1.5403430461883545, 'logits/rejected': -1.5628353357315063, 'epoch': 1.0673234811165846}\n",
      "{'loss': 0.2996, 'grad_norm': 7.972540378570557, 'learning_rate': 3.085659551176793e-05, 'rewards/chosen': 0.5146457552909851, 'rewards/rejected': -1.337246060371399, 'rewards/accuracies': 0.8899999856948853, 'rewards/margins': 1.8518917560577393, 'logps/chosen': -32.300071716308594, 'logps/rejected': -72.05774688720703, 'logits/chosen': -1.8204962015151978, 'logits/rejected': -1.853644847869873, 'epoch': 1.1494252873563218}\n",
      "{'loss': 0.3091, 'grad_norm': 1.130771517753601, 'learning_rate': 2.948823207443897e-05, 'rewards/chosen': 0.4853556454181671, 'rewards/rejected': -1.4657264947891235, 'rewards/accuracies': 0.8675000071525574, 'rewards/margins': 1.9510821104049683, 'logps/chosen': -33.55998611450195, 'logps/rejected': -74.3216323852539, 'logits/chosen': -1.824302077293396, 'logits/rejected': -1.8945667743682861, 'epoch': 1.2315270935960592}\n",
      "{'loss': 0.2878, 'grad_norm': 35.87119674682617, 'learning_rate': 2.8119868637110018e-05, 'rewards/chosen': 0.40995463728904724, 'rewards/rejected': -1.589913010597229, 'rewards/accuracies': 0.8799999952316284, 'rewards/margins': 1.9998676776885986, 'logps/chosen': -34.88874053955078, 'logps/rejected': -77.08143615722656, 'logits/chosen': -2.002638578414917, 'logits/rejected': -2.0459835529327393, 'epoch': 1.3136288998357963}\n",
      "{'loss': 0.3058, 'grad_norm': 25.563920974731445, 'learning_rate': 2.6751505199781062e-05, 'rewards/chosen': 0.38685446977615356, 'rewards/rejected': -1.4969379901885986, 'rewards/accuracies': 0.8849999904632568, 'rewards/margins': 1.8837924003601074, 'logps/chosen': -35.0266227722168, 'logps/rejected': -75.32562255859375, 'logits/chosen': -2.019752025604248, 'logits/rejected': -2.028439998626709, 'epoch': 1.3957307060755337}\n",
      "{'loss': 0.2749, 'grad_norm': 8.243054389953613, 'learning_rate': 2.5383141762452106e-05, 'rewards/chosen': 0.5539425611495972, 'rewards/rejected': -1.4489867687225342, 'rewards/accuracies': 0.8899999856948853, 'rewards/margins': 2.002929449081421, 'logps/chosen': -31.796768188476562, 'logps/rejected': -74.7538833618164, 'logits/chosen': -2.096717119216919, 'logits/rejected': -2.135545253753662, 'epoch': 1.477832512315271}\n",
      "{'loss': 0.2724, 'grad_norm': 8.296576499938965, 'learning_rate': 2.4014778325123154e-05, 'rewards/chosen': 0.5428016781806946, 'rewards/rejected': -1.6315617561340332, 'rewards/accuracies': 0.875, 'rewards/margins': 2.174363136291504, 'logps/chosen': -31.619504928588867, 'logps/rejected': -77.51244354248047, 'logits/chosen': -2.2325451374053955, 'logits/rejected': -2.2195751667022705, 'epoch': 1.5599343185550083}\n",
      "{'loss': 0.2773, 'grad_norm': 11.010054588317871, 'learning_rate': 2.2646414887794198e-05, 'rewards/chosen': 0.512460470199585, 'rewards/rejected': -1.8052711486816406, 'rewards/accuracies': 0.8774999976158142, 'rewards/margins': 2.3177316188812256, 'logps/chosen': -32.44065475463867, 'logps/rejected': -81.57199096679688, 'logits/chosen': -2.243446111679077, 'logits/rejected': -2.2926278114318848, 'epoch': 1.6420361247947455}\n",
      "{'loss': 0.2567, 'grad_norm': 7.251445293426514, 'learning_rate': 2.1278051450465245e-05, 'rewards/chosen': 0.4788510799407959, 'rewards/rejected': -1.7382255792617798, 'rewards/accuracies': 0.9049999713897705, 'rewards/margins': 2.217076539993286, 'logps/chosen': -33.314083099365234, 'logps/rejected': -80.48760986328125, 'logits/chosen': -2.181018352508545, 'logits/rejected': -2.1970789432525635, 'epoch': 1.7241379310344827}\n",
      "{'loss': 0.2699, 'grad_norm': 5.5034356117248535, 'learning_rate': 1.990968801313629e-05, 'rewards/chosen': 0.5840190052986145, 'rewards/rejected': -1.6088485717773438, 'rewards/accuracies': 0.8700000047683716, 'rewards/margins': 2.1928675174713135, 'logps/chosen': -30.877885818481445, 'logps/rejected': -77.28143310546875, 'logits/chosen': -2.2350504398345947, 'logits/rejected': -2.247579574584961, 'epoch': 1.80623973727422}\n",
      "{'loss': 0.3474, 'grad_norm': 3.321418285369873, 'learning_rate': 1.8541324575807333e-05, 'rewards/chosen': 0.4337792694568634, 'rewards/rejected': -1.4946209192276, 'rewards/accuracies': 0.8500000238418579, 'rewards/margins': 1.9284003973007202, 'logps/chosen': -34.00190353393555, 'logps/rejected': -74.50760650634766, 'logits/chosen': -2.269132614135742, 'logits/rejected': -2.2817108631134033, 'epoch': 1.8883415435139574}\n",
      "{'loss': 0.2851, 'grad_norm': 12.039728164672852, 'learning_rate': 1.717296113847838e-05, 'rewards/chosen': 0.4081308841705322, 'rewards/rejected': -1.7158422470092773, 'rewards/accuracies': 0.8949999809265137, 'rewards/margins': 2.1239731311798096, 'logps/chosen': -34.45338821411133, 'logps/rejected': -79.32518768310547, 'logits/chosen': -2.2953782081604004, 'logits/rejected': -2.3077187538146973, 'epoch': 1.9704433497536946}\n",
      "{'loss': 0.231, 'grad_norm': 3.4214673042297363, 'learning_rate': 1.5804597701149425e-05, 'rewards/chosen': 0.44698527455329895, 'rewards/rejected': -1.8552509546279907, 'rewards/accuracies': 0.9125000238418579, 'rewards/margins': 2.302236318588257, 'logps/chosen': -33.797203063964844, 'logps/rejected': -82.40303039550781, 'logits/chosen': -2.35257625579834, 'logits/rejected': -2.3694753646850586, 'epoch': 2.052545155993432}\n",
      "{'loss': 0.2669, 'grad_norm': 10.38033390045166, 'learning_rate': 1.4436234263820472e-05, 'rewards/chosen': 0.3850668966770172, 'rewards/rejected': -1.9235670566558838, 'rewards/accuracies': 0.8799999952316284, 'rewards/margins': 2.308633804321289, 'logps/chosen': -35.36372375488281, 'logps/rejected': -83.80807495117188, 'logits/chosen': -2.4500930309295654, 'logits/rejected': -2.480344533920288, 'epoch': 2.134646962233169}\n",
      "{'loss': 0.2709, 'grad_norm': 4.539989471435547, 'learning_rate': 1.3067870826491516e-05, 'rewards/chosen': 0.1887359768152237, 'rewards/rejected': -2.1287646293640137, 'rewards/accuracies': 0.8849999904632568, 'rewards/margins': 2.317500591278076, 'logps/chosen': -39.0863151550293, 'logps/rejected': -87.4724349975586, 'logits/chosen': -2.424983501434326, 'logits/rejected': -2.4158589839935303, 'epoch': 2.2167487684729066}\n",
      "{'loss': 0.2149, 'grad_norm': 8.350428581237793, 'learning_rate': 1.1699507389162562e-05, 'rewards/chosen': 0.3038077652454376, 'rewards/rejected': -2.172727584838867, 'rewards/accuracies': 0.9225000143051147, 'rewards/margins': 2.4765353202819824, 'logps/chosen': -36.68376159667969, 'logps/rejected': -88.69503784179688, 'logits/chosen': -2.42179799079895, 'logits/rejected': -2.4352731704711914, 'epoch': 2.2988505747126435}\n",
      "{'loss': 0.2512, 'grad_norm': 15.525163650512695, 'learning_rate': 1.0331143951833607e-05, 'rewards/chosen': 0.4238603115081787, 'rewards/rejected': -2.0299551486968994, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 2.453815460205078, 'logps/chosen': -33.64409637451172, 'logps/rejected': -85.53921508789062, 'logits/chosen': -2.374617099761963, 'logits/rejected': -2.373098373413086, 'epoch': 2.380952380952381}\n",
      "{'loss': 0.2634, 'grad_norm': 3.131394624710083, 'learning_rate': 8.962780514504653e-06, 'rewards/chosen': 0.40278685092926025, 'rewards/rejected': -2.012099504470825, 'rewards/accuracies': 0.9024999737739563, 'rewards/margins': 2.414886474609375, 'logps/chosen': -34.97864532470703, 'logps/rejected': -85.65186309814453, 'logits/chosen': -2.4498443603515625, 'logits/rejected': -2.4712867736816406, 'epoch': 2.4630541871921183}\n",
      "{'loss': 0.2953, 'grad_norm': 4.29957389831543, 'learning_rate': 7.594417077175699e-06, 'rewards/chosen': 0.20009905099868774, 'rewards/rejected': -2.0778138637542725, 'rewards/accuracies': 0.8575000166893005, 'rewards/margins': 2.2779130935668945, 'logps/chosen': -38.819366455078125, 'logps/rejected': -86.21719360351562, 'logits/chosen': -2.4622485637664795, 'logits/rejected': -2.451361894607544, 'epoch': 2.5451559934318553}\n",
      "{'loss': 0.2349, 'grad_norm': 3.8268961906433105, 'learning_rate': 6.226053639846744e-06, 'rewards/chosen': 0.452027291059494, 'rewards/rejected': -2.0516090393066406, 'rewards/accuracies': 0.9150000214576721, 'rewards/margins': 2.503636360168457, 'logps/chosen': -33.55853271484375, 'logps/rejected': -86.11805725097656, 'logits/chosen': -2.4902994632720947, 'logits/rejected': -2.493093490600586, 'epoch': 2.6272577996715927}\n",
      "{'loss': 0.2585, 'grad_norm': 5.866939544677734, 'learning_rate': 4.857690202517789e-06, 'rewards/chosen': 0.33288246393203735, 'rewards/rejected': -1.9905871152877808, 'rewards/accuracies': 0.887499988079071, 'rewards/margins': 2.323469638824463, 'logps/chosen': -36.32435607910156, 'logps/rejected': -85.16838073730469, 'logits/chosen': -2.462193727493286, 'logits/rejected': -2.461038589477539, 'epoch': 2.70935960591133}\n",
      "{'loss': 0.2248, 'grad_norm': 5.55238676071167, 'learning_rate': 3.4893267651888343e-06, 'rewards/chosen': 0.5300034284591675, 'rewards/rejected': -2.0250072479248047, 'rewards/accuracies': 0.9100000262260437, 'rewards/margins': 2.5550107955932617, 'logps/chosen': -31.830286026000977, 'logps/rejected': -85.87464141845703, 'logits/chosen': -2.494957208633423, 'logits/rejected': -2.5796031951904297, 'epoch': 2.7914614121510675}\n",
      "{'loss': 0.1991, 'grad_norm': 13.35391902923584, 'learning_rate': 2.1209633278598796e-06, 'rewards/chosen': 0.5010195970535278, 'rewards/rejected': -2.10843563079834, 'rewards/accuracies': 0.9100000262260437, 'rewards/margins': 2.609454870223999, 'logps/chosen': -33.00777816772461, 'logps/rejected': -87.85177612304688, 'logits/chosen': -2.5278196334838867, 'logits/rejected': -2.540719985961914, 'epoch': 2.873563218390805}\n",
      "{'loss': 0.234, 'grad_norm': 15.789762496948242, 'learning_rate': 7.52599890530925e-07, 'rewards/chosen': 0.4361758530139923, 'rewards/rejected': -2.093451738357544, 'rewards/accuracies': 0.9175000190734863, 'rewards/margins': 2.529627799987793, 'logps/chosen': -33.81230926513672, 'logps/rejected': -87.57333374023438, 'logits/chosen': -2.4611284732818604, 'logits/rejected': -2.490901470184326, 'epoch': 2.955665024630542}\n",
      "{'train_runtime': 273.9063, 'train_samples_per_second': 53.35, 'train_steps_per_second': 13.34, 'train_loss': 0.3249337648392768, 'epoch': 3.0}\n",
      "done.\n",
      "Running benchmark for Qwen2.5-3B-Instruct\n",
      "Running pipeline: baseline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:08<00:00,  4.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n",
      "Running pipeline: few_shot_sweep...\n",
      "Running configuration 1...\n",
      "Running configuration 2...\n",
      "Running configuration 3...\n",
      "Running configuration 4...\n",
      "done.\n",
      "Running pipeline: dpo_lora...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.39s/it]\n",
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 52075.60 examples/s]\n",
      "Extracting prompt in train dataset: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 33352.63 examples/s]\n",
      "Applying chat template to train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 38893.30 examples/s]\n",
      "Tokenizing train dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4871/4871 [00:00<00:00, 6516.01 examples/s]\n",
      "Train dataset reference log probs: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1218/1218 [05:52<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6791, 'grad_norm': 0.911645770072937, 'learning_rate': 4.8645320197044334e-05, 'rewards/chosen': 0.06905547529459, 'rewards/rejected': 0.03922340273857117, 'rewards/accuracies': 0.7024999856948853, 'rewards/margins': 0.029832076281309128, 'logps/chosen': -45.35773849487305, 'logps/rejected': -49.17695236206055, 'logits/chosen': -0.46724268794059753, 'logits/rejected': -0.4864496886730194, 'epoch': 0.08210180623973727}\n",
      "{'loss': 0.5751, 'grad_norm': 1.7376508712768555, 'learning_rate': 4.7276956759715385e-05, 'rewards/chosen': 0.27644240856170654, 'rewards/rejected': -0.013218633830547333, 'rewards/accuracies': 0.7975000143051147, 'rewards/margins': 0.2896610200405121, 'logps/chosen': -41.33415985107422, 'logps/rejected': -50.232948303222656, 'logits/chosen': -2.039604425430298, 'logits/rejected': -2.055771827697754, 'epoch': 0.16420361247947454}\n",
      "{'loss': 0.4719, 'grad_norm': 3.441840171813965, 'learning_rate': 4.590859332238643e-05, 'rewards/chosen': 0.39632648229599, 'rewards/rejected': -0.2430996298789978, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 0.6394261121749878, 'logps/chosen': -39.18722152709961, 'logps/rejected': -54.669090270996094, 'logits/chosen': -2.882737398147583, 'logits/rejected': -3.0957465171813965, 'epoch': 0.24630541871921183}\n",
      "{'loss': 0.4197, 'grad_norm': 3.108534812927246, 'learning_rate': 4.454022988505747e-05, 'rewards/chosen': 0.2688354253768921, 'rewards/rejected': -0.6423432230949402, 'rewards/accuracies': 0.8525000214576721, 'rewards/margins': 0.9111786484718323, 'logps/chosen': -41.86983871459961, 'logps/rejected': -62.94182586669922, 'logits/chosen': -2.6789886951446533, 'logits/rejected': -2.9862215518951416, 'epoch': 0.3284072249589491}\n",
      "{'loss': 0.3743, 'grad_norm': 6.310470104217529, 'learning_rate': 4.3171866447728524e-05, 'rewards/chosen': 0.25836941599845886, 'rewards/rejected': -0.9265280365943909, 'rewards/accuracies': 0.8650000095367432, 'rewards/margins': 1.184897541999817, 'logps/chosen': -42.50371170043945, 'logps/rejected': -69.14221954345703, 'logits/chosen': -2.399322271347046, 'logits/rejected': -2.7766835689544678, 'epoch': 0.41050903119868637}\n",
      "{'loss': 0.3514, 'grad_norm': 2.9352290630340576, 'learning_rate': 4.180350301039956e-05, 'rewards/chosen': 0.19827167689800262, 'rewards/rejected': -1.2092422246932983, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 1.4075138568878174, 'logps/chosen': -43.076847076416016, 'logps/rejected': -74.17131805419922, 'logits/chosen': -2.4024646282196045, 'logits/rejected': -2.8973119258880615, 'epoch': 0.49261083743842365}\n",
      "{'loss': 0.2947, 'grad_norm': 13.400629997253418, 'learning_rate': 4.0435139573070605e-05, 'rewards/chosen': 0.04808935523033142, 'rewards/rejected': -1.6819865703582764, 'rewards/accuracies': 0.9075000286102295, 'rewards/margins': 1.7300758361816406, 'logps/chosen': -45.708885192871094, 'logps/rejected': -83.74249267578125, 'logits/chosen': -2.3352291584014893, 'logits/rejected': -2.882469415664673, 'epoch': 0.5747126436781609}\n",
      "{'loss': 0.3224, 'grad_norm': 2.081406831741333, 'learning_rate': 3.9066776135741656e-05, 'rewards/chosen': 0.00611517671495676, 'rewards/rejected': -1.7274760007858276, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 1.7335909605026245, 'logps/chosen': -46.806541442871094, 'logps/rejected': -84.22164916992188, 'logits/chosen': -2.098435401916504, 'logits/rejected': -2.790309429168701, 'epoch': 0.6568144499178982}\n",
      "{'loss': 0.3158, 'grad_norm': 2.958548069000244, 'learning_rate': 3.76984126984127e-05, 'rewards/chosen': 0.0364706888794899, 'rewards/rejected': -1.843287467956543, 'rewards/accuracies': 0.8675000071525574, 'rewards/margins': 1.8797581195831299, 'logps/chosen': -46.33891296386719, 'logps/rejected': -86.67588806152344, 'logits/chosen': -2.1485977172851562, 'logits/rejected': -2.8965399265289307, 'epoch': 0.7389162561576355}\n",
      "{'loss': 0.2948, 'grad_norm': 6.126672744750977, 'learning_rate': 3.6330049261083744e-05, 'rewards/chosen': -0.013872734270989895, 'rewards/rejected': -1.9816181659698486, 'rewards/accuracies': 0.8849999904632568, 'rewards/margins': 1.9677456617355347, 'logps/chosen': -47.062950134277344, 'logps/rejected': -89.36392211914062, 'logits/chosen': -2.3025195598602295, 'logits/rejected': -3.2103686332702637, 'epoch': 0.8210180623973727}\n",
      "{'loss': 0.2941, 'grad_norm': 6.342485427856445, 'learning_rate': 3.4961685823754795e-05, 'rewards/chosen': 0.01140839233994484, 'rewards/rejected': -1.9891928434371948, 'rewards/accuracies': 0.875, 'rewards/margins': 2.000601053237915, 'logps/chosen': -46.9945182800293, 'logps/rejected': -89.61982727050781, 'logits/chosen': -2.175204277038574, 'logits/rejected': -3.0280492305755615, 'epoch': 0.90311986863711}\n",
      "{'loss': 0.3178, 'grad_norm': 5.521353244781494, 'learning_rate': 3.359332238642583e-05, 'rewards/chosen': -0.09053774178028107, 'rewards/rejected': -2.1668238639831543, 'rewards/accuracies': 0.862500011920929, 'rewards/margins': 2.0762860774993896, 'logps/chosen': -49.21636199951172, 'logps/rejected': -93.4292984008789, 'logits/chosen': -2.1776740550994873, 'logits/rejected': -2.990147590637207, 'epoch': 0.9852216748768473}\n",
      "{'loss': 0.2685, 'grad_norm': 2.9234628677368164, 'learning_rate': 3.222495894909688e-05, 'rewards/chosen': 0.004485730081796646, 'rewards/rejected': -2.2815041542053223, 'rewards/accuracies': 0.8866667151451111, 'rewards/margins': 2.285989761352539, 'logps/chosen': -46.73891067504883, 'logps/rejected': -95.5070571899414, 'logits/chosen': -2.171651840209961, 'logits/rejected': -3.0381178855895996, 'epoch': 1.0673234811165846}\n",
      "{'loss': 0.2418, 'grad_norm': 5.711855411529541, 'learning_rate': 3.085659551176793e-05, 'rewards/chosen': 0.009078502655029297, 'rewards/rejected': -2.4309427738189697, 'rewards/accuracies': 0.9049999713897705, 'rewards/margins': 2.440021276473999, 'logps/chosen': -46.533931732177734, 'logps/rejected': -98.22765350341797, 'logits/chosen': -2.241610527038574, 'logits/rejected': -3.199465751647949, 'epoch': 1.1494252873563218}\n",
      "{'loss': 0.2457, 'grad_norm': 0.9901449084281921, 'learning_rate': 2.948823207443897e-05, 'rewards/chosen': -0.16617141664028168, 'rewards/rejected': -2.687983751296997, 'rewards/accuracies': 0.8924999833106995, 'rewards/margins': 2.521812677383423, 'logps/chosen': -50.71247100830078, 'logps/rejected': -103.8627700805664, 'logits/chosen': -2.3307089805603027, 'logits/rejected': -3.291473627090454, 'epoch': 1.2315270935960592}\n",
      "{'loss': 0.2445, 'grad_norm': 16.369709014892578, 'learning_rate': 2.8119868637110018e-05, 'rewards/chosen': -0.09200969338417053, 'rewards/rejected': -2.6118853092193604, 'rewards/accuracies': 0.9049999713897705, 'rewards/margins': 2.5198755264282227, 'logps/chosen': -48.953094482421875, 'logps/rejected': -101.99520874023438, 'logits/chosen': -2.3082571029663086, 'logits/rejected': -3.3775634765625, 'epoch': 1.3136288998357963}\n",
      "{'loss': 0.2465, 'grad_norm': 11.408224105834961, 'learning_rate': 2.6751505199781062e-05, 'rewards/chosen': -0.019145222380757332, 'rewards/rejected': -2.4572255611419678, 'rewards/accuracies': 0.9150000214576721, 'rewards/margins': 2.438080310821533, 'logps/chosen': -47.36000061035156, 'logps/rejected': -99.26567077636719, 'logits/chosen': -2.522179365158081, 'logits/rejected': -3.497892379760742, 'epoch': 1.3957307060755337}\n",
      "{'loss': 0.2346, 'grad_norm': 3.0449509620666504, 'learning_rate': 2.5383141762452106e-05, 'rewards/chosen': 0.03463030979037285, 'rewards/rejected': -2.561398983001709, 'rewards/accuracies': 0.9075000286102295, 'rewards/margins': 2.596029043197632, 'logps/chosen': -46.683040618896484, 'logps/rejected': -101.43196105957031, 'logits/chosen': -2.5117971897125244, 'logits/rejected': -3.4006166458129883, 'epoch': 1.477832512315271}\n",
      "{'loss': 0.2189, 'grad_norm': 4.9143147468566895, 'learning_rate': 2.4014778325123154e-05, 'rewards/chosen': 0.07390421628952026, 'rewards/rejected': -2.6775317192077637, 'rewards/accuracies': 0.9100000262260437, 'rewards/margins': 2.751436233520508, 'logps/chosen': -45.45615768432617, 'logps/rejected': -103.677978515625, 'logits/chosen': -2.446878433227539, 'logits/rejected': -3.5319759845733643, 'epoch': 1.5599343185550083}\n",
      "{'loss': 0.2061, 'grad_norm': 10.286544799804688, 'learning_rate': 2.2646414887794198e-05, 'rewards/chosen': -0.12108836323022842, 'rewards/rejected': -3.053598403930664, 'rewards/accuracies': 0.9125000238418579, 'rewards/margins': 2.9325101375579834, 'logps/chosen': -49.22820281982422, 'logps/rejected': -110.82532501220703, 'logits/chosen': -2.561051845550537, 'logits/rejected': -3.507967472076416, 'epoch': 1.6420361247947455}\n",
      "{'loss': 0.2214, 'grad_norm': 5.361452102661133, 'learning_rate': 2.1278051450465245e-05, 'rewards/chosen': 0.04220036044716835, 'rewards/rejected': -2.8968400955200195, 'rewards/accuracies': 0.9049999713897705, 'rewards/margins': 2.939040422439575, 'logps/chosen': -46.28764343261719, 'logps/rejected': -108.23426818847656, 'logits/chosen': -2.591362237930298, 'logits/rejected': -3.511859655380249, 'epoch': 1.7241379310344827}\n",
      "{'loss': 0.2176, 'grad_norm': 22.08680534362793, 'learning_rate': 1.990968801313629e-05, 'rewards/chosen': 0.021570386365056038, 'rewards/rejected': -2.9589507579803467, 'rewards/accuracies': 0.8974999785423279, 'rewards/margins': 2.9805212020874023, 'logps/chosen': -46.86817932128906, 'logps/rejected': -109.2851791381836, 'logits/chosen': -2.8076653480529785, 'logits/rejected': -3.7597849369049072, 'epoch': 1.80623973727422}\n",
      "{'loss': 0.2815, 'grad_norm': 10.136324882507324, 'learning_rate': 1.8541324575807333e-05, 'rewards/chosen': -0.1347654163837433, 'rewards/rejected': -2.7149171829223633, 'rewards/accuracies': 0.8700000047683716, 'rewards/margins': 2.5801515579223633, 'logps/chosen': -49.655731201171875, 'logps/rejected': -104.11463165283203, 'logits/chosen': -2.807321071624756, 'logits/rejected': -3.584468126296997, 'epoch': 1.8883415435139574}\n",
      "{'loss': 0.2271, 'grad_norm': 5.597937107086182, 'learning_rate': 1.717296113847838e-05, 'rewards/chosen': -0.0023425042163580656, 'rewards/rejected': -2.8381574153900146, 'rewards/accuracies': 0.9175000190734863, 'rewards/margins': 2.835815191268921, 'logps/chosen': -47.328765869140625, 'logps/rejected': -106.75652313232422, 'logits/chosen': -2.9070253372192383, 'logits/rejected': -3.7504770755767822, 'epoch': 1.9704433497536946}\n",
      "{'loss': 0.1982, 'grad_norm': 2.739798069000244, 'learning_rate': 1.5804597701149425e-05, 'rewards/chosen': 0.02471257373690605, 'rewards/rejected': -2.9653971195220947, 'rewards/accuracies': 0.9200000166893005, 'rewards/margins': 2.990109443664551, 'logps/chosen': -46.483192443847656, 'logps/rejected': -109.30552673339844, 'logits/chosen': -2.907822847366333, 'logits/rejected': -3.796419382095337, 'epoch': 2.052545155993432}\n",
      "{'loss': 0.1951, 'grad_norm': 1.2607604265213013, 'learning_rate': 1.4436234263820472e-05, 'rewards/chosen': 0.1106785386800766, 'rewards/rejected': -3.0668821334838867, 'rewards/accuracies': 0.9300000071525574, 'rewards/margins': 3.177560329437256, 'logps/chosen': -44.822235107421875, 'logps/rejected': -111.14413452148438, 'logits/chosen': -2.8120551109313965, 'logits/rejected': -3.6624906063079834, 'epoch': 2.134646962233169}\n",
      "{'loss': 0.2093, 'grad_norm': 0.5865138173103333, 'learning_rate': 1.3067870826491516e-05, 'rewards/chosen': 0.03893618658185005, 'rewards/rejected': -2.989438056945801, 'rewards/accuracies': 0.9200000166893005, 'rewards/margins': 3.028373956680298, 'logps/chosen': -46.18611145019531, 'logps/rejected': -109.29463958740234, 'logits/chosen': -2.9015684127807617, 'logits/rejected': -3.7566986083984375, 'epoch': 2.2167487684729066}\n",
      "{'loss': 0.1719, 'grad_norm': 13.904221534729004, 'learning_rate': 1.1699507389162562e-05, 'rewards/chosen': 0.02222234196960926, 'rewards/rejected': -3.1435024738311768, 'rewards/accuracies': 0.9325000047683716, 'rewards/margins': 3.165724754333496, 'logps/chosen': -46.93350601196289, 'logps/rejected': -113.16461944580078, 'logits/chosen': -2.9223132133483887, 'logits/rejected': -3.684166669845581, 'epoch': 2.2988505747126435}\n",
      "{'loss': 0.1801, 'grad_norm': 16.939830780029297, 'learning_rate': 1.0331143951833607e-05, 'rewards/chosen': 0.0816037729382515, 'rewards/rejected': -3.172398090362549, 'rewards/accuracies': 0.9325000047683716, 'rewards/margins': 3.2540013790130615, 'logps/chosen': -44.92607116699219, 'logps/rejected': -113.34483337402344, 'logits/chosen': -2.922750949859619, 'logits/rejected': -3.702448844909668, 'epoch': 2.380952380952381}\n",
      "{'loss': 0.2021, 'grad_norm': 3.343125820159912, 'learning_rate': 8.962780514504653e-06, 'rewards/chosen': 0.11553004384040833, 'rewards/rejected': -3.177079677581787, 'rewards/accuracies': 0.9375, 'rewards/margins': 3.292609930038452, 'logps/chosen': -44.760623931884766, 'logps/rejected': -113.62959289550781, 'logits/chosen': -2.951406240463257, 'logits/rejected': -3.6548612117767334, 'epoch': 2.4630541871921183}\n",
      "{'loss': 0.1986, 'grad_norm': 7.602869510650635, 'learning_rate': 7.594417077175699e-06, 'rewards/chosen': 0.0506012924015522, 'rewards/rejected': -3.0813148021698, 'rewards/accuracies': 0.925000011920929, 'rewards/margins': 3.131915807723999, 'logps/chosen': -46.1992073059082, 'logps/rejected': -111.36370086669922, 'logits/chosen': -3.043288230895996, 'logits/rejected': -3.6929898262023926, 'epoch': 2.5451559934318553}\n",
      "{'loss': 0.19, 'grad_norm': 4.165827751159668, 'learning_rate': 6.226053639846744e-06, 'rewards/chosen': 0.1144801527261734, 'rewards/rejected': -3.142277240753174, 'rewards/accuracies': 0.9175000190734863, 'rewards/margins': 3.2567574977874756, 'logps/chosen': -44.809722900390625, 'logps/rejected': -112.6462173461914, 'logits/chosen': -3.059427499771118, 'logits/rejected': -3.793018102645874, 'epoch': 2.6272577996715927}\n",
      "{'loss': 0.1875, 'grad_norm': 7.708434104919434, 'learning_rate': 4.857690202517789e-06, 'rewards/chosen': 0.06671817600727081, 'rewards/rejected': -3.05845308303833, 'rewards/accuracies': 0.9325000047683716, 'rewards/margins': 3.125171422958374, 'logps/chosen': -46.30402755737305, 'logps/rejected': -111.51763916015625, 'logits/chosen': -3.0558974742889404, 'logits/rejected': -3.792222261428833, 'epoch': 2.70935960591133}\n",
      "{'loss': 0.1764, 'grad_norm': 10.10222053527832, 'learning_rate': 3.4893267651888343e-06, 'rewards/chosen': 0.2377060353755951, 'rewards/rejected': -3.086087942123413, 'rewards/accuracies': 0.9325000047683716, 'rewards/margins': 3.323794364929199, 'logps/chosen': -41.99028396606445, 'logps/rejected': -111.56341552734375, 'logits/chosen': -3.0443427562713623, 'logits/rejected': -3.8176429271698, 'epoch': 2.7914614121510675}\n",
      "{'loss': 0.1603, 'grad_norm': 8.93895435333252, 'learning_rate': 2.1209633278598796e-06, 'rewards/chosen': 0.21468256413936615, 'rewards/rejected': -3.1055896282196045, 'rewards/accuracies': 0.9424999952316284, 'rewards/margins': 3.3202714920043945, 'logps/chosen': -42.871185302734375, 'logps/rejected': -112.76464080810547, 'logits/chosen': -3.0339672565460205, 'logits/rejected': -3.772146701812744, 'epoch': 2.873563218390805}\n",
      "{'loss': 0.1553, 'grad_norm': 1.874045491218567, 'learning_rate': 7.52599890530925e-07, 'rewards/chosen': 0.1954442858695984, 'rewards/rejected': -3.116598129272461, 'rewards/accuracies': 0.9300000071525574, 'rewards/margins': 3.312042474746704, 'logps/chosen': -43.060089111328125, 'logps/rejected': -112.38351440429688, 'logits/chosen': -3.054360866546631, 'logits/rejected': -3.8347887992858887, 'epoch': 2.955665024630542}\n",
      "{'train_runtime': 395.9861, 'train_samples_per_second': 36.903, 'train_steps_per_second': 9.228, 'train_loss': 0.27061017693622424, 'epoch': 3.0}\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "all_profiles = {}\n",
    "\n",
    "for model_name in MODELS:\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    print(f\"Running benchmark for {short_name}\")\n",
    "\n",
    "    dpo_lora = create_dpo_control(model_name)\n",
    "\n",
    "    benchmark = Benchmark(\n",
    "        use_case=commonsense_mcqa,\n",
    "        base_model_name_or_path=model_name,\n",
    "        steering_pipelines={\n",
    "            \"baseline\": [],\n",
    "            \"few_shot_sweep\": [few_shot_spec],\n",
    "            \"dpo_lora\": [dpo_lora],\n",
    "        },\n",
    "        gen_kwargs={\"max_new_tokens\": 300, \"do_sample\": True, \"temperature\": 0.7},\n",
    "        device_map=\"auto\",\n",
    "        num_trials=5\n",
    "    )\n",
    "\n",
    "    profiles = benchmark.run()\n",
    "    all_profiles[short_name] = profiles\n",
    "\n",
    "    # export per-model results\n",
    "    benchmark.export(profiles, save_dir=f\"./profiles/{short_name}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bb6f43-ba41-4117-a366-6580013620bc",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We now analyze the benchmark results across all model sizes. With multiple trials, we can compute mean and standard deviation to understand the statistical reliability of our comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten_section",
   "metadata": {},
   "source": [
    "First, we flatten the nested profiles into a single DataFrame with one row per trial, then aggregate across trials to get mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86624b5e-6b67-413f-87b5-9acf241f39b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>pipeline</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>k_positive</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>positional_bias</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.0788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.0736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.1168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.1044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.1008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.1092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.1056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.1036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.1076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.1060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.1100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model        pipeline  trial_id  k_positive  accuracy  \\\n",
       "0   Qwen2.5-0.5B-Instruct        baseline         0         NaN      0.32   \n",
       "1   Qwen2.5-0.5B-Instruct        baseline         1         NaN      0.40   \n",
       "2   Qwen2.5-0.5B-Instruct        baseline         2         NaN      0.32   \n",
       "3   Qwen2.5-0.5B-Instruct        baseline         3         NaN      0.38   \n",
       "4   Qwen2.5-0.5B-Instruct        baseline         4         NaN      0.32   \n",
       "5   Qwen2.5-0.5B-Instruct  few_shot_sweep         0         1.0      0.44   \n",
       "6   Qwen2.5-0.5B-Instruct  few_shot_sweep         1         1.0      0.40   \n",
       "7   Qwen2.5-0.5B-Instruct  few_shot_sweep         2         1.0      0.44   \n",
       "8   Qwen2.5-0.5B-Instruct  few_shot_sweep         3         1.0      0.44   \n",
       "9   Qwen2.5-0.5B-Instruct  few_shot_sweep         4         1.0      0.38   \n",
       "10  Qwen2.5-0.5B-Instruct  few_shot_sweep         0         5.0      0.40   \n",
       "11  Qwen2.5-0.5B-Instruct  few_shot_sweep         1         5.0      0.38   \n",
       "12  Qwen2.5-0.5B-Instruct  few_shot_sweep         2         5.0      0.48   \n",
       "13  Qwen2.5-0.5B-Instruct  few_shot_sweep         3         5.0      0.38   \n",
       "14  Qwen2.5-0.5B-Instruct  few_shot_sweep         4         5.0      0.44   \n",
       "\n",
       "    positional_bias  \n",
       "0            0.0652  \n",
       "1            0.0744  \n",
       "2            0.0692  \n",
       "3            0.0788  \n",
       "4            0.0736  \n",
       "5            0.1168  \n",
       "6            0.1044  \n",
       "7            0.1008  \n",
       "8            0.1092  \n",
       "9            0.1096  \n",
       "10           0.1056  \n",
       "11           0.1036  \n",
       "12           0.1076  \n",
       "13           0.1060  \n",
       "14           0.1100  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = []\n",
    "for model_name, profiles in all_profiles.items():\n",
    "    df = flatten_profiles(\n",
    "        profiles,\n",
    "        metric_accessors={\n",
    "            \"accuracy\": (\"MCQAAccuracy\", \"question_mean\"),\n",
    "            \"positional_bias\": (\"MCQAPositionalBias\", \"mean\"),\n",
    "        }\n",
    "    )\n",
    "    df[\"model\"] = model_name\n",
    "    df[\"k_positive\"] = get_param_values(df, \"FewShot\", \"k_positive\")\n",
    "    dfs.append(df)\n",
    "\n",
    "runs_df = pd.concat(dfs, ignore_index=True)\n",
    "runs_df[[\"model\", \"pipeline\", \"trial_id\", \"k_positive\", \"accuracy\", \"positional_bias\"]].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "summarize_cell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>pipeline</th>\n",
       "      <th>k_positive</th>\n",
       "      <th>n_trials</th>\n",
       "      <th>accuracy_mean</th>\n",
       "      <th>accuracy_std</th>\n",
       "      <th>positional_bias_mean</th>\n",
       "      <th>positional_bias_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.348</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Qwen2.5-0.5B-Instruct</td>\n",
       "      <td>dpo_lora</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.416</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Qwen2.5-1.5B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Qwen2.5-1.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Qwen2.5-1.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Qwen2.5-1.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Qwen2.5-1.5B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Qwen2.5-1.5B-Instruct</td>\n",
       "      <td>dpo_lora</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>baseline</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.752</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>few_shot_sweep</td>\n",
       "      <td>25.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Qwen2.5-3B-Instruct</td>\n",
       "      <td>dpo_lora</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    model        pipeline  k_positive  n_trials  \\\n",
       "0   Qwen2.5-0.5B-Instruct        baseline         NaN       5.0   \n",
       "1   Qwen2.5-0.5B-Instruct  few_shot_sweep         1.0       5.0   \n",
       "2   Qwen2.5-0.5B-Instruct  few_shot_sweep         5.0       5.0   \n",
       "3   Qwen2.5-0.5B-Instruct  few_shot_sweep        10.0       5.0   \n",
       "4   Qwen2.5-0.5B-Instruct  few_shot_sweep        25.0       5.0   \n",
       "5   Qwen2.5-0.5B-Instruct        dpo_lora         NaN       5.0   \n",
       "6   Qwen2.5-1.5B-Instruct        baseline         NaN       5.0   \n",
       "7   Qwen2.5-1.5B-Instruct  few_shot_sweep         1.0       5.0   \n",
       "8   Qwen2.5-1.5B-Instruct  few_shot_sweep         5.0       5.0   \n",
       "9   Qwen2.5-1.5B-Instruct  few_shot_sweep        10.0       5.0   \n",
       "10  Qwen2.5-1.5B-Instruct  few_shot_sweep        25.0       5.0   \n",
       "11  Qwen2.5-1.5B-Instruct        dpo_lora         NaN       5.0   \n",
       "12    Qwen2.5-3B-Instruct        baseline         NaN       5.0   \n",
       "13    Qwen2.5-3B-Instruct  few_shot_sweep         1.0       5.0   \n",
       "14    Qwen2.5-3B-Instruct  few_shot_sweep         5.0       5.0   \n",
       "15    Qwen2.5-3B-Instruct  few_shot_sweep        10.0       5.0   \n",
       "16    Qwen2.5-3B-Instruct  few_shot_sweep        25.0       5.0   \n",
       "17    Qwen2.5-3B-Instruct        dpo_lora         NaN       5.0   \n",
       "\n",
       "    accuracy_mean  accuracy_std  positional_bias_mean  positional_bias_std  \n",
       "0           0.348         0.039                 0.072                0.005  \n",
       "1           0.420         0.028                 0.108                0.006  \n",
       "2           0.416         0.043                 0.107                0.002  \n",
       "3           0.444         0.026                 0.103                0.003  \n",
       "4           0.420         0.049                 0.102                0.003  \n",
       "5           0.416         0.026                 0.116                0.003  \n",
       "6           0.760         0.014                 0.012                0.004  \n",
       "7           0.772         0.011                 0.028                0.006  \n",
       "8           0.760         0.014                 0.030                0.006  \n",
       "9           0.772         0.011                 0.023                0.004  \n",
       "10          0.764         0.009                 0.020                0.005  \n",
       "11          0.756         0.022                 0.017                0.002  \n",
       "12          0.752         0.018                 0.023                0.006  \n",
       "13          0.772         0.036                 0.032                0.004  \n",
       "14          0.764         0.022                 0.027                0.005  \n",
       "15          0.764         0.022                 0.027                0.003  \n",
       "16          0.764         0.030                 0.024                0.004  \n",
       "17          0.728         0.027                 0.034                0.006  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summarize by configuration (aggregate across trials)\n",
    "summary_df = summarize_by_config(\n",
    "    runs_df,\n",
    "    metric_cols=[\"accuracy\", \"positional_bias\"],\n",
    "    group_cols=[\"model\", \"pipeline\", \"config_id\"]\n",
    ")\n",
    "\n",
    "# add k_positive for few-shot rows\n",
    "k_map = runs_df.groupby([\"model\", \"pipeline\", \"config_id\"])[\"k_positive\"].first()\n",
    "summary_df[\"k_positive\"] = summary_df.apply(\n",
    "    lambda row: k_map.get((row[\"model\"], row[\"pipeline\"], row[\"config_id\"]), np.nan), axis=1\n",
    ")\n",
    "\n",
    "summary_df[[\"model\", \"pipeline\", \"k_positive\", \"n_trials\", \"accuracy_mean\", \"accuracy_std\", \"positional_bias_mean\", \"positional_bias_std\"]].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewshot_scaling_section",
   "metadata": {},
   "source": [
    "### FewShot scaling\n",
    "\n",
    "We examine how FewShot accuracy scales with the number of positive examples. The baseline (unsteered) and DPO-LoRA results are shown as horizontal reference lines for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewshot_scaling_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_df = summary_df[summary_df[\"pipeline\"] == \"few_shot_sweep\"].copy()\n",
    "few_shot_df = few_shot_df.sort_values([\"model\", \"k_positive\"])\n",
    "\n",
    "# compute common axis limits for consistency across models\n",
    "all_accuracy = runs_df[\"accuracy\"].dropna()\n",
    "ylim_accuracy = (max(0, all_accuracy.min() - 0.1), min(1, all_accuracy.max() + 0.1))\n",
    "\n",
    "n_models = len(MODELS)\n",
    "fig = plt.figure(figsize=(5 * n_models, 4))\n",
    "gs = gridspec.GridSpec(1, n_models, wspace=0.3)\n",
    "\n",
    "for idx, model_name in enumerate(MODELS):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "\n",
    "    # data for this model\n",
    "    model_swept = few_shot_df[few_shot_df[\"model\"] == short_name].copy()\n",
    "    model_baseline = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"baseline\")]\n",
    "    model_trials = runs_df[(runs_df[\"model\"] == short_name) & (runs_df[\"pipeline\"] == \"few_shot_sweep\")]\n",
    "\n",
    "    plot_sensitivity(\n",
    "        swept=model_swept,\n",
    "        metric=\"accuracy\",\n",
    "        sweep_col=\"k_positive\",\n",
    "        baseline=model_baseline,\n",
    "        per_trial_data=model_trials,\n",
    "        ax=ax,\n",
    "        metric_label=\"accuracy\",\n",
    "        sweep_label=\"k_positive\",\n",
    "        title=short_name,\n",
    "        ylim=ylim_accuracy,\n",
    "        save_path=FIGURE_DIR / f\"sensitivity_accuracy_{short_name}.png\" if idx == 0 else None,\n",
    "    )\n",
    "\n",
    "    # add DPO-LoRA reference line\n",
    "    dpo_row = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"dpo_lora\")]\n",
    "    if not dpo_row.empty:\n",
    "        dpo_acc = dpo_row[\"accuracy_mean\"].iloc[0]\n",
    "        dpo_std = dpo_row[\"accuracy_std\"].iloc[0]\n",
    "        ax.axhline(dpo_acc, color=\"#E24A33\", linestyle=\":\", linewidth=1.5, label=\"DPO-LoRA\")\n",
    "        ax.axhspan(dpo_acc - dpo_std, dpo_acc + dpo_std, color=\"#E24A33\", alpha=0.1)\n",
    "        ax.legend(frameon=False, loc=\"lower right\", fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tradeoff_section",
   "metadata": {},
   "source": [
    "### Accuracy vs positional bias tradeoff\n",
    "\n",
    "We examine whether there is a tradeoff between accuracy and positional bias across methods. The FewShot configurations are colored by `k_positive`, with the baseline shown as a black X marker and DPO-LoRA as a red square. The Pareto frontier indicates configurations that are not dominated by any other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tradeoff_scatter_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute common axis limits for consistency across models\n",
    "all_accuracy = runs_df[\"accuracy\"].dropna()\n",
    "all_bias = runs_df[\"positional_bias\"].dropna()\n",
    "xlim_tradeoff = (max(0, all_accuracy.min() - 0.05), min(1, all_accuracy.max() + 0.05))\n",
    "ylim_tradeoff = (max(0, all_bias.min() - 0.02), all_bias.max() + 0.02)\n",
    "\n",
    "n_models = len(MODELS)\n",
    "fig = plt.figure(figsize=(5 * n_models, 5))\n",
    "gs = gridspec.GridSpec(1, n_models, wspace=0.3)\n",
    "\n",
    "for idx, model_name in enumerate(MODELS):\n",
    "    short_name = model_name.split(\"/\")[-1]\n",
    "    ax = fig.add_subplot(gs[0, idx])\n",
    "\n",
    "    # data for this model\n",
    "    model_swept = few_shot_df[few_shot_df[\"model\"] == short_name].copy()\n",
    "    model_baseline = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"baseline\")]\n",
    "    model_trials = runs_df[(runs_df[\"model\"] == short_name) & (runs_df[\"pipeline\"] == \"few_shot_sweep\")]\n",
    "\n",
    "    plot_tradeoff(\n",
    "        swept=model_swept,\n",
    "        x_metric=\"accuracy\",\n",
    "        y_metric=\"positional_bias\",\n",
    "        sweep_col=\"k_positive\",\n",
    "        baseline=model_baseline,\n",
    "        per_trial_data=model_trials,\n",
    "        ax=ax,\n",
    "        x_label=\"accuracy\",\n",
    "        y_label=\"positional bias\",\n",
    "        sweep_label=\"k_positive\",\n",
    "        title=short_name,\n",
    "        show_pareto=True,\n",
    "        maximize_x=True,\n",
    "        maximize_y=False,  # lower positional bias is better\n",
    "        xlim=xlim_tradeoff,\n",
    "        ylim=ylim_tradeoff,\n",
    "        save_path=FIGURE_DIR / f\"tradeoff_{short_name}.png\" if idx == 0 else None,\n",
    "    )\n",
    "\n",
    "    # add DPO-LoRA marker\n",
    "    dpo_row = summary_df[(summary_df[\"model\"] == short_name) & (summary_df[\"pipeline\"] == \"dpo_lora\")]\n",
    "    if not dpo_row.empty:\n",
    "        dpo_x = dpo_row[\"accuracy_mean\"].iloc[0]\n",
    "        dpo_y = dpo_row[\"positional_bias_mean\"].iloc[0]\n",
    "        dpo_x_err = dpo_row[\"accuracy_std\"].iloc[0]\n",
    "        dpo_y_err = dpo_row[\"positional_bias_std\"].iloc[0]\n",
    "        ax.errorbar(dpo_x, dpo_y, xerr=dpo_x_err, yerr=dpo_y_err, fmt=\"none\", ecolor=\"#E24A33\", elinewidth=0.5, capsize=2, capthick=0.5, zorder=6)\n",
    "        ax.scatter(dpo_x, dpo_y, marker=\"s\", s=100, c=\"#E24A33\", edgecolors=\"white\", linewidth=1, zorder=7, label=\"DPO-LoRA\")\n",
    "        ax.legend(frameon=False, loc=\"upper right\", fontsize=8)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary_section",
   "metadata": {},
   "source": [
    "### Summary table\n",
    "\n",
    "The table below summarizes all configurations ranked by accuracy, providing a comprehensive view of performance across methods and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad9a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define method ordering (benchmark order)\n",
    "method_order = [\"baseline\", \"FewShot (k=1)\", \"FewShot (k=5)\", \"FewShot (k=10)\", \"FewShot (k=25)\", \"DPO-LoRA\"]\n",
    "\n",
    "summary_table = summary_df.copy()\n",
    "summary_table[\"method\"] = summary_table.apply(\n",
    "    lambda row: \"baseline\" if row[\"pipeline\"] == \"baseline\"\n",
    "    else \"DPO-LoRA\" if row[\"pipeline\"] == \"dpo_lora\"\n",
    "    else f\"FewShot (k={int(row['k_positive'])})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# create ordering keys for proper sorting\n",
    "model_order = [m.split(\"/\")[-1] for m in MODELS]\n",
    "summary_table[\"model_order\"] = summary_table[\"model\"].apply(lambda m: model_order.index(m) if m in model_order else len(model_order))\n",
    "summary_table[\"method_order\"] = summary_table[\"method\"].apply(lambda m: method_order.index(m) if m in method_order else len(method_order))\n",
    "\n",
    "display_df = summary_table.sort_values([\"model_order\", \"method_order\"])[\n",
    "    [\"model\", \"method\", \"n_trials\", \"accuracy_mean\", \"accuracy_std\", \"positional_bias_mean\", \"positional_bias_std\"]\n",
    "].copy()\n",
    "display_df.columns = [\"model\", \"method\", \"trials\", \"accuracy (mean)\", \"accuracy (std)\", \"pos bias (mean)\", \"pos bias (std)\"]\n",
    "\n",
    "display_df.style.format({\n",
    "    \"accuracy (mean)\": \"{:.1%}\",\n",
    "    \"accuracy (std)\": \"{:.1%}\",\n",
    "    \"pos bias (mean)\": \"{:.3f}\",\n",
    "    \"pos bias (std)\": \"{:.3f}\",\n",
    "}).background_gradient(subset=[\"accuracy (mean)\"], cmap=\"RdYlGn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "takeaways_section",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "Few-shot prompting provides a lightweight alternative to fine-tuning for improving commonsense reasoning. Even a small number of positive examples can yield meaningful accuracy gains over the baseline. DPO-trained LoRA adapters offer comparable performance but require more upfront compute for training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b38f543-8b1c-4bbe-bf9a-5e7fec484701",
   "metadata": {},
   "source": [
    "# Running MergeKit methods\n",
    "\n",
    "The toolkit implements [MergeKit](https://github.com/arcee-ai/mergekit) methods via a `StructuralControl` wrapper. Methods are initialized via either a `config_dict` or a `config_path` (to a `yaml` file). Since merging results in a model, the option `lazy_init=True` must be set when creating a `SteeringPipeline` (rather than passing in `model_name_or_path`). This notebook outlines how to construct some of MergeKit's methods in our toolkit; for a more complete list of implementations enabled by MergeKit please see the [example configs](https://github.com/arcee-ai/mergekit/tree/main/examples) and the [documentation](https://github.com/arcee-ai/mergekit/blob/main/docs/merge_methods.md).\n",
    "\n",
    "**Note**: Please note that the toolkit depends on MergeKit 0.0.5.1 (due to more restrictive licenses of more recent versions). As a result, not all recent MergeKit methods are available in our toolkit. Merging operations can be resource and storage intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927fd15",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43064a0",
   "metadata": {},
   "source": [
    "If running this from a Google Colab notebook, please uncomment the following cell to install the toolkit. The following block is not necessary if running this notebook from a virtual environment where the package has already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4504747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/IBM/AISteer360.git\n",
    "# %cd AISteer360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7279897f",
   "metadata": {},
   "source": [
    "The following authentication steps may be necessary to access any gated models (after being granted access by Hugging Face). Uncomment the following if you need to log in to the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff25b9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# from huggingface_hub import login\n",
    "# login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f49f52-0dc2-480c-8d9d-93018ba041f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "from aisteer360.algorithms.structural_control.wrappers.mergekit import MergeKit\n",
    "\n",
    "prompt = \"Who was the fifth president of the United States?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf3191e",
   "metadata": {},
   "source": [
    "The following authentication steps may be necessary to access any gated models (even after being granted access by Hugging Face). Uncomment the following if you need to log in to the Hugging Face Hub using your token stored in the `.env` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9728bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# from huggingface_hub import login\n",
    "# login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2bbe7-5a27-4822-a8e8-a8757b5e85c2",
   "metadata": {},
   "source": [
    "## Linear merge\n",
    "\n",
    "Linear merge is a method that combines multiple models by averaging their weights (see the [original paper](https://arxiv.org/abs/2203.05482) for details). To run this method via MergeKit, specify the source models (to average) and associated scalar weights. Note that the weights are not required to sum to one as weights are scaled appropriately internally.\n",
    "\n",
    "The config below creates a float16 model by weighted-averaging corresponding tensors from three 13B models. Orca Mini v3 (`weight=1.0`) is the dominant contributor, Wizard 13B v1.2 adds a moderate influence (`weight=0.5`), and WizardLM contributes lightly (`weight=0.3`). \n",
    "\n",
    "The final parameters are proportional to the `models[].parameters.weight` values (i.e., a normalized blend)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7d3388-61b8-41a2-aebe-55548dc02d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|███████████████████████████| 7/7 [00:31<00:00,  4.47s/it]\n",
      "Fetching 11 files: 100%|████████████████████████| 11/11 [00:30<00:00,  2.79s/it]\n",
      "Warmup loader cache: 100%|████████████████████████| 2/2 [01:12<00:00, 36.25s/it]\n",
      "Executing graph: 100%|██████████████████████| 1817/1817 [02:36<00:00, 11.61it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [00:41<00:00,  6.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response (linear merge):\n",
      " \n",
      "\n",
      "James Monroe was the fifth president of the United States.\n"
     ]
    }
   ],
   "source": [
    "linear_merge_config = {\n",
    "    \"merge_method\": \"linear\",\n",
    "    \"dtype\": \"float16\",\n",
    "    \"models\": [\n",
    "        {\"model\": \"pankajmathur/orca_mini_v3_13b\", \"parameters\": {\"weight\": 0.5}},\n",
    "        {\"model\": \"WizardLMTeam/WizardLM-13B-V1.2\", \"parameters\": {\"weight\": 0.5}},\n",
    "    ],\n",
    "}\n",
    "\n",
    "linear_merge = MergeKit(\n",
    "    config_dict=linear_merge_config,\n",
    "    out_path=\"./mergekit_models/orca-wizard-blend-linear\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# create steering pipeline\n",
    "linear_merge_pipeline = SteeringPipeline(\n",
    "    lazy_init=True,  # required when calling MergeKit methods\n",
    "    controls=[linear_merge],\n",
    "    device=\"cuda\"\n",
    ")\n",
    "linear_merge_pipeline.steer()\n",
    "\n",
    "# inference\n",
    "tokenizer = linear_merge_pipeline.tokenizer\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "\n",
    "gen_params = {\n",
    "    \"max_new_tokens\": 500\n",
    "}\n",
    "\n",
    "steered_response = linear_merge_pipeline.generate_text(\n",
    "    inputs.input_ids,\n",
    "    **gen_params\n",
    ")\n",
    "print(\"Response (linear merge):\\n\", *steered_response)  # mergekit returns a list of responses; unpack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486325aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional cleanup\n",
    "import shutil\n",
    "shutil.rmtree(\"./mergekit_models/orca-wizard-blend-linear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c26608-58b9-4c92-b023-d49a97eeab36",
   "metadata": {},
   "source": [
    "## SLERP merge\n",
    "\n",
    "SLERP (spherical linear interpolation) merge is a method that combines model weights by moving along the surface of a high‑dimensional hypersphere with the goal of yielding a merged model that better preserves scale and source model behaviors.\n",
    "\n",
    "The setup below builds on Orca Mini v3 as the `base_model` and merges it with Wizard 13B v1.2 over `slices[0].sources` spanning `layer_range=[0,40]`. Instead of a straight average, it uses spherical linear interpolation, controlled by `parameters.t` schedules: attention blocks (`filter=\"self_attn\"`) follow a layerwise t pattern `[0, 0.5, 0.3, 0.7, 1]`, MLP blocks (`filter=\"mlp\"`) use `[1, 0.5, 0.7, 0.3, 0]`, and everything else defaults to `t=0.5`. \n",
    "\n",
    "The resulting model is a float16 hybrid where attention and MLP mix ratios vary across depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0402379b-3cd8-4b64-8e12-c6bff1bdf192",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warmup loader cache: 100%|█████████████████████| 2/2 [00:00<00:00, 30174.85it/s]\n",
      "Executing graph: 100%|██████████████████████| 1817/1817 [03:05<00:00,  9.78it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [00:49<00:00,  8.21s/it]\n",
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response (SLERP merge):\n",
      " \n",
      "The fifth president of the United States was James Monroe. He was in office from 1817 to 1825.\n"
     ]
    }
   ],
   "source": [
    "slerp_merge_config = {\n",
    "    \"merge_method\": \"slerp\",\n",
    "    \"dtype\": \"float16\",\n",
    "    \"base_model\": \"pankajmathur/orca_mini_v3_13b\",\n",
    "    \"slices\": [\n",
    "        {\n",
    "            \"sources\": [\n",
    "                {\"model\": \"pankajmathur/orca_mini_v3_13b\", \"layer_range\": [0, 40]},\n",
    "                {\"model\": \"WizardLMTeam/WizardLM-13B-V1.2\", \"layer_range\": [0, 40]},\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"parameters\": {\n",
    "        \"t\": [\n",
    "            {\"filter\": \"self_attn\", \"value\": [0, 0.5, 0.3, 0.7, 1]},\n",
    "            {\"filter\": \"mlp\", \"value\": [1, 0.5, 0.7, 0.3, 0]},\n",
    "            {\"value\": 0.5},\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "slerp_merge = MergeKit(\n",
    "    config_dict=slerp_merge_config,\n",
    "    out_path=\"./mergekit_models/orca-wizard-blend-slerp\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "slerp_merge_pipeline = SteeringPipeline(\n",
    "    lazy_init=True,\n",
    "    controls=[slerp_merge],\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "slerp_merge_pipeline.steer()\n",
    "\n",
    "tokenizer = slerp_merge_pipeline.tokenizer\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "gen_params = {\n",
    "    \"max_new_tokens\": 500\n",
    "}\n",
    "\n",
    "steered_response = slerp_merge_pipeline.generate_text(\n",
    "    inputs.input_ids,\n",
    "    **gen_params\n",
    ")\n",
    "print(\"Response (SLERP merge):\\n\", *steered_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca5598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional cleanup\n",
    "import shutil\n",
    "shutil.rmtree(\"./mergekit_models/orca-wizard-blend-slerp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58b0267-1d7b-4b1c-9e93-1ae799c70b56",
   "metadata": {},
   "source": [
    "## TIES merge\n",
    "\n",
    "The [TIES method](https://proceedings.neurips.cc/paper_files/paper/2023/file/1644c9af28ab7916874f6fd6228a9bcf-Paper-Conference.pdf) merges models by first identifying/removing any redundant parameters across models, selecting the most important parameters (via a vote), resolving sign conflicts, and finally merging the aligned parameters to create a unified multi-task model.\n",
    "\n",
    "The setup below produces a sparse, float16 hybrid on top of Llama-2-13B using TIES selection rather than full blending. Global `parameters` enable `normalize=True` (scale alignment) and `int8_mask=True` (efficient sparsity masking). Per-model controls set what fraction to keep (`density`) and how strongly to scale (`weight`), optionally varying by layer or module:\n",
    "\n",
    "* Orca Mini v3 `density=[1, 0.7, 0.1]` (keep most early, little late), `weight=1.0`.\n",
    "* Platypus2 `density=0.5`, `weight=[0, 0.3, 0.7, 1]` (growing influence with depth).\n",
    "* WizardLM `density=0.33`, `weight=[{\"filter\":\"mlp\",\"value\":0.5},{\"value\":0}]` (only MLPs contribute at 0.5; others ignored).\n",
    "\n",
    "The result is a model that retains the strongest weights from each source with layer-/module-aware sparsity and scaling.\n",
    "\n",
    "Note: TIES merging can be computationally intensive to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5490b009-933c-4f6b-8bba-09bbefbfecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 11 files: 100%|████████████████████████| 11/11 [00:26<00:00,  2.37s/it]\n",
      "Fetching 10 files: 100%|████████████████████████| 10/10 [02:20<00:00, 14.04s/it]\n",
      "Warmup loader cache: 100%|████████████████████████| 4/4 [02:46<00:00, 41.68s/it]\n",
      "Executing graph: 100%|██████████████████████| 2543/2543 [41:24<00:00,  1.02it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [00:41<00:00,  6.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response (TIES merge):\n",
      " \n",
      "James Monroe was the fifth president of the United States. He served from 1817 to 1825.\n"
     ]
    }
   ],
   "source": [
    "ties_merge_config = {\n",
    "    \"merge_method\": \"ties\",\n",
    "    \"dtype\": \"float16\",\n",
    "    \"base_model\": \"TheBloke/Llama-2-13B-fp16\",\n",
    "    \"parameters\": {\n",
    "        \"normalize\": True,\n",
    "        \"int8_mask\": True,\n",
    "    },\n",
    "    \"models\": [\n",
    "        {\n",
    "            \"model\": \"pankajmathur/orca_mini_v3_13b\",\n",
    "            \"parameters\": {\n",
    "                \"density\": [1, 0.7, 0.1],\n",
    "                \"weight\": 1.0,\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"garage-bAInd/Platypus2-13B\",\n",
    "            \"parameters\": {\n",
    "                \"density\": 0.5,\n",
    "                \"weight\": [0, 0.3, 0.7, 1],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"model\": \"WizardLMTeam/WizardLM-13B-V1.2\",\n",
    "            \"parameters\": {\n",
    "                \"density\": 0.33,\n",
    "                \"weight\": [\n",
    "                    {\"filter\": \"mlp\", \"value\": 0.5},\n",
    "                    {\"value\": 0},\n",
    "                ],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "}\n",
    "\n",
    "ties_merge = MergeKit(\n",
    "    config_dict=ties_merge_config,\n",
    "    out_path=\"./mergekit_models/llama-orca-platypus-wizard-blend-ties\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "ties_merge_pipeline = SteeringPipeline(\n",
    "    lazy_init=True,\n",
    "    controls=[ties_merge],\n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "ties_merge_pipeline.steer()\n",
    "\n",
    "tokenizer = ties_merge_pipeline.tokenizer\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "gen_params = {\n",
    "    \"max_new_tokens\": 500\n",
    "}\n",
    "\n",
    "steered_response = ties_merge_pipeline.generate_text(\n",
    "    inputs.input_ids,\n",
    "    **gen_params\n",
    ")\n",
    "print(\"Response (TIES merge):\\n\", *steered_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0b6d0-44f8-44d9-876b-af9ec4f7e023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional cleanup\n",
    "import shutil\n",
    "shutil.rmtree(\"./mergekit_models/llama-orca-platypus-wizard-blend-ties\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

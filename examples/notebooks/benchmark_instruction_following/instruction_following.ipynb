{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb23221b",
   "metadata": {},
   "source": [
    "# Studying Steering Side-Effects\n",
    "\n",
    "In this notebook, we study the instruction following ability of a model across a range of instruction types. Additionally, we inspect if steering the model to be better at following instructions impacts the model's response quality in general."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f24a6b5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ee4eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "from aisteer360.algorithms.state_control.pasta.control import PASTA\n",
    "from aisteer360.algorithms.core.specs import ControlSpec\n",
    "from aisteer360.evaluation.use_cases.instruction_following import InstructionFollowing\n",
    "from aisteer360.evaluation.metrics.custom.instruction_following.strict_instruction import StrictInstruction\n",
    "from aisteer360.evaluation.metrics.generic.reward_score import RewardScore\n",
    "from aisteer360.evaluation.benchmark import Benchmark\n",
    "from aisteer360.evaluation.utils import (\n",
    "    flatten_profiles,\n",
    "    summarize_by_config,\n",
    "    get_param_values,\n",
    "    build_per_example_df,\n",
    "    to_jsonable,\n",
    ")\n",
    "from aisteer360.evaluation.utils.viz_utils import (\n",
    "    create_tradeoff_figure,\n",
    "    plot_metric_heatmap,\n",
    "    plot_comparison_bars,\n",
    "    plot_pareto_frontier,\n",
    "    plot_tradeoff_scatter,\n",
    ")\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c08b25",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "There are innumerable types of instructions that a model can be prompted with. To better understand a model's instruction following ability in general, we explore model behavior across a wide range of instruction types as organized by the `IFEval` dataset. For the purposes of this study, we make use of our modified version of the IFEval dataset, termed `Split-IFEval`, in which the instructions are explicitly extracted from the prompt (this makes it easier to create interventions that rely directly on these tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f0593",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifeval = load_dataset(\"ibm-research/Split-IFEval\")\n",
    "ifeval_df = ifeval[\"train\"].to_pandas()\n",
    "\n",
    "cols = [\"instructions\", \"instruction_id_list\", \"kwargs\"]\n",
    "for col in cols:\n",
    "    ifeval_df[col] = ifeval_df[col].apply(\n",
    "        lambda x: x.tolist() if isinstance(x, np.ndarray) else x\n",
    "    )\n",
    "\n",
    "ifeval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70868e5f",
   "metadata": {},
   "source": [
    "Notice via the `instruction_id_list` column, each prompt can in general contain a number of instructions. We'll focus on the prompts that contain a single example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dda876",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifeval_df[\"num_instructions\"] = ifeval_df[\"instruction_id_list\"].apply(len)\n",
    "single_instr_df = ifeval_df[ifeval_df[\"num_instructions\"] == 1].copy()\n",
    "single_instr_df[\"instruction_id\"] = single_instr_df[\"instruction_id_list\"].apply(lambda ids: ids[0])\n",
    "instruction_group_sizes = (\n",
    "    single_instr_df[\"instruction_id\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"instruction_id\")\n",
    "    .reset_index(name=\"count\")\n",
    ")\n",
    "\n",
    "instruction_group_sizes.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414e527",
   "metadata": {},
   "source": [
    "We'll study the following instruction types:\n",
    "\n",
    "- `keywords:forbidden_words`: describes that the response must avoid using anything from the specified forbidden list.\n",
    "- `detectable_format:number_highlighted_sections`: describes that the response must contain at least a specified number of highlighted sections using a defined markup pattern.\n",
    "- `language:response_language`: indicates that the model must generate its entire response in a specific target language.\n",
    "- `startend:end_checker`: describes that the response must end with an exact required phrase (with nothing extra following it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c264723",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_types = [\n",
    "    \"keywords:forbidden_words\",\n",
    "    \"detectable_format:number_highlighted_sections\",\n",
    "    \"language:response_language\",\n",
    "    \"startend:end_checker\",\n",
    "]\n",
    "\n",
    "filtered_df = single_instr_df[\n",
    "    single_instr_df[\"instruction_id\"].isin(instruction_types)\n",
    "].copy()\n",
    "\n",
    "balanced_filtered = (\n",
    "    filtered_df.groupby(\"instruction_id\")\n",
    "    .apply(lambda g: g.sample(min(len(g), 12), random_state=123))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "balanced_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f8bc8",
   "metadata": {},
   "source": [
    "Evaluation data takes the form of a prompt (including instructions), the specific instructions (separated from the prompt), the IDs of the instructions, and any associated kwargs for the instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98cb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_data = [\n",
    "    {\n",
    "        \"prompt\": row[\"prompt\"],\n",
    "        \"instructions\": to_jsonable(row[\"instructions\"]),\n",
    "        \"instruction_id_list\": to_jsonable(row[\"instruction_id_list\"]),\n",
    "        \"kwargs\": to_jsonable(row[\"kwargs\"]),\n",
    "    }\n",
    "    for _, row in balanced_filtered.iterrows()\n",
    "]\n",
    "\n",
    "len(evaluation_data), evaluation_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b90a4d",
   "metadata": {},
   "source": [
    "## Defining the benchmark\n",
    "\n",
    "We use the `ControlSpec` class to sweep the steering strength `alpha`. The impacted layers and the method are assumed to be fixed throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3168ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_spec = ControlSpec(\n",
    "    control_cls=PASTA,\n",
    "    params={\n",
    "        \"head_config\": list(range(8, 24)),\n",
    "        \"scale_position\": \"include\",\n",
    "    },\n",
    "    vars=[\n",
    "        {\"alpha\": 5.0},\n",
    "        {\"alpha\": 10.0},\n",
    "        {\"alpha\": 20.0},\n",
    "        {\"alpha\": 30.0},\n",
    "        {\"alpha\": 40.0},\n",
    "        {\"alpha\": 50.0},\n",
    "    ],\n",
    "    name=\"PASTA\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769c8c2",
   "metadata": {},
   "source": [
    "The instruction following use case is initialized with two metrics: `StrictInstruction` and `RewardScore`. We will be studying the trade-off between these two metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf60fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_following = InstructionFollowing(\n",
    "    evaluation_data=evaluation_data,\n",
    "    evaluation_metrics=[\n",
    "        StrictInstruction(),\n",
    "        RewardScore(\n",
    "            model_or_id=\"OpenAssistant/reward-model-deberta-v3-large-v2\",\n",
    "            score_transform=\"identity\",\n",
    "            batch_size=8,\n",
    "            max_length=1024,\n",
    "            return_logits=False,\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a76c1a",
   "metadata": {},
   "source": [
    "The benchmark can then be defined on two steering pipelines: the baseline (unsteered) model, and the above `pasta_spec`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f57ff5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark = Benchmark(\n",
    "    use_case=instruction_following,\n",
    "    base_model_name_or_path=MODEL_NAME,\n",
    "    steering_pipelines={\n",
    "        \"baseline\": [],\n",
    "        \"pasta_alpha_sweep\": [pasta_spec],\n",
    "    },\n",
    "    runtime_overrides={\n",
    "        \"PASTA\": {\"substrings\": \"instructions\"},\n",
    "    },\n",
    "    gen_kwargs={\n",
    "        \"max_new_tokens\": 128,\n",
    "        \"do_sample\": True,\n",
    "        \"output_attentions\": True,\n",
    "    },\n",
    "    hf_model_kwargs={\n",
    "        \"attn_implementation\": \"eager\",\n",
    "    },\n",
    "    device_map=\"auto\",\n",
    "    num_trials=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788fe7e9",
   "metadata": {},
   "source": [
    "Running the benchmark yields the profiles across the baseline and the full set of configurations in the spec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585faa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles = benchmark.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5ed3a",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "We now use the built-in utilities from `aisteer360.evaluation.utils` to process and visualize the profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flatten_section",
   "metadata": {},
   "source": [
    "### Flattening profiles\n",
    "\n",
    "The `flatten_profiles` utility converts the nested benchmark output into a flat DataFrame with one row per run. We specify which metrics to extract via `metric_accessors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dc24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten profiles with metric extraction\n",
    "runs_df = flatten_profiles(\n",
    "    profiles,\n",
    "    metric_accessors={\n",
    "        \"strict_prompt_acc\": (\"StrictInstruction\", \"strict_prompt_accuracy\"),\n",
    "        \"strict_instr_acc\": (\"StrictInstruction\", \"strict_instruction_accuracy\"),\n",
    "        \"mean_reward\": (\"RewardScore\", \"mean_reward\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "# extract the swept alpha parameter as a column\n",
    "runs_df[\"alpha\"] = get_param_values(runs_df, \"PASTA\", \"alpha\")\n",
    "\n",
    "display(\n",
    "    runs_df\n",
    "    .drop(columns=[\"_run\", \"params\"])\n",
    "    .sort_values([\"alpha\", \"trial_id\"], na_position=\"last\")\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summarize_section",
   "metadata": {},
   "source": [
    "### Summarizing by configuration\n",
    "\n",
    "The `summarize_by_config` utility aggregates metrics across trials, computing mean and std for each configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2624fc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize across trials\n",
    "summary = summarize_by_config(\n",
    "    runs_df,\n",
    "    metric_cols=[\"strict_prompt_acc\", \"strict_instr_acc\", \"mean_reward\"],\n",
    "    group_cols=[\"pipeline\", \"config_id\", \"alpha\"],\n",
    ")\n",
    "\n",
    "# add a readable config label\n",
    "summary[\"config\"] = summary[\"alpha\"].apply(\n",
    "    lambda a: \"baseline\" if pd.isna(a) else f\"alpha={a}\"\n",
    ")\n",
    "\n",
    "display(summary[[\n",
    "    \"config\", \"alpha\", \"n_trials\",\n",
    "    \"strict_prompt_acc_mean\", \"strict_prompt_acc_std\",\n",
    "    \"mean_reward_mean\", \"mean_reward_std\"\n",
    "]].sort_values(\"alpha\", na_position=\"last\").round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f82480",
   "metadata": {},
   "source": [
    "### Tradeoff visualization\n",
    "\n",
    "The `create_tradeoff_figure` utility generates a 3-panel figure showing:\n",
    "\n",
    "1. Instruction following rate vs steering strength\n",
    "2. Output quality (reward) vs steering strength  \n",
    "3. Scatter plot of reward vs accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c9d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = create_tradeoff_figure(\n",
    "    summary,\n",
    "    x_metric=\"strict_prompt_acc\",\n",
    "    y_metric=\"mean_reward\",\n",
    "    sweep_col=\"alpha\",\n",
    "    baseline_pipeline=\"baseline\",\n",
    "    x_label=\"Strict Prompt Accuracy\",\n",
    "    y_label=\"Mean Reward Score\",\n",
    "    sweep_label=\"Alpha\",\n",
    "    save_path=\"pasta_tradeoff_analysis.png\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "example_comparison_section",
   "metadata": {},
   "source": [
    "### Per-example analysis\n",
    "\n",
    "The `build_per_example_df` utility extracts per-example results from a single run, making it easy to analyze individual cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef0248",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_by_config(runs_df: pd.DataFrame, pipeline: str, alpha=None, trial_id: int = 0):\n",
    "    \"\"\"Get a specific run from the flattened DataFrame.\"\"\"\n",
    "    if pipeline == \"baseline\":\n",
    "        mask = (runs_df[\"pipeline\"] == \"baseline\") & (runs_df[\"trial_id\"] == trial_id)\n",
    "    else:\n",
    "        mask = (runs_df[\"pipeline\"] == pipeline) & (runs_df[\"alpha\"] == alpha) & (runs_df[\"trial_id\"] == trial_id)\n",
    "    return runs_df.loc[mask, \"_run\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad996853",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_summary = summary[summary[\"config\"] != \"baseline\"]\n",
    "strongest_alpha = pasta_summary[\"alpha\"].min()\n",
    "\n",
    "baseline_run = get_run_by_config(runs_df, \"baseline\")\n",
    "strong_run = get_run_by_config(runs_df, \"pasta_alpha_sweep\", strongest_alpha)\n",
    "\n",
    "baseline_ex = build_per_example_df(\n",
    "    baseline_run,\n",
    "    generation_fields=[\"prompt\", \"response\", \"instruction_id_list\"],\n",
    "    metric_lists={\n",
    "        \"followed\": (\"StrictInstruction\", \"follow_all_instructions\"),\n",
    "        \"reward\": (\"RewardScore\", \"rewards\"),\n",
    "    }\n",
    ")\n",
    "strong_ex = build_per_example_df(\n",
    "    strong_run,\n",
    "    generation_fields=[\"prompt\", \"response\", \"instruction_id_list\"],\n",
    "    metric_lists={\n",
    "        \"followed\": (\"StrictInstruction\", \"follow_all_instructions\"),\n",
    "        \"reward\": (\"RewardScore\", \"rewards\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "# find cases where steering fixed instruction following\n",
    "comparison = baseline_ex[[\"idx\", \"followed\", \"reward\"]].merge(\n",
    "    strong_ex[[\"idx\", \"followed\", \"reward\"]],\n",
    "    on=\"idx\", suffixes=(\"_base\", \"_strong\")\n",
    ")\n",
    "fixed = comparison[(~comparison[\"followed_base\"]) & (comparison[\"followed_strong\"])].copy()\n",
    "fixed[\"reward_delta\"] = fixed[\"reward_strong\"] - fixed[\"reward_base\"]\n",
    "\n",
    "fixed.sort_values(\"reward_delta\")[[\"idx\", \"reward_base\", \"reward_strong\", \"reward_delta\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74002bd7",
   "metadata": {},
   "source": [
    "## Additional plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d5900d",
   "metadata": {},
   "source": [
    "### Per-instruction-type breakdown\n",
    "\n",
    "We can use `plot_metric_heatmap` to visualize performance across instruction types and steering strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_per_instruction_results(profiles, evaluation_data):\n",
    "    \"\"\"Break down results by instruction type and alpha.\"\"\"\n",
    "    rows = []\n",
    "\n",
    "    for pipeline_name, runs in profiles.items():\n",
    "        for run in runs:\n",
    "            alpha = (run.get(\"params\", {}) or {}).get(\"PASTA\", {}).get(\"alpha\", None)\n",
    "            if pipeline_name == \"baseline\":\n",
    "                alpha = 0.0\n",
    "\n",
    "            generations = run[\"generations\"]\n",
    "            followed_list = run[\"evaluations\"][\"StrictInstruction\"][\"follow_all_instructions\"]\n",
    "            rewards = run[\"evaluations\"][\"RewardScore\"][\"rewards\"]\n",
    "\n",
    "            for i, (gen, followed, reward) in enumerate(zip(generations, followed_list, rewards)):\n",
    "                instr_id = gen[\"instruction_id_list\"][0] if gen.get(\"instruction_id_list\") else None\n",
    "                rows.append({\n",
    "                    \"alpha\": alpha,\n",
    "                    \"instruction_type\": instr_id.split(\":\")[-1] if instr_id else None,\n",
    "                    \"followed\": followed,\n",
    "                    \"reward\": reward,\n",
    "                    \"trial_id\": run[\"trial_id\"],\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "per_instr_df = extract_per_instruction_results(profiles, evaluation_data)\n",
    "\n",
    "# aggregate by instruction type and alpha\n",
    "instr_summary = (\n",
    "    per_instr_df\n",
    "    .groupby([\"instruction_type\", \"alpha\"])\n",
    "    .agg(\n",
    "        follow_rate=(\"followed\", \"mean\"),\n",
    "        mean_reward=(\"reward\", \"mean\"),\n",
    "        n=(\"followed\", \"count\")\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ceca68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "follow_pivot = instr_summary.pivot(index=\"instruction_type\", columns=\"alpha\", values=\"follow_rate\")\n",
    "reward_pivot = instr_summary.pivot(index=\"instruction_type\", columns=\"alpha\", values=\"mean_reward\")\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    follow_pivot,\n",
    "    ax=axes[0],\n",
    "    title=\"instruction following by type & alpha\",\n",
    "    xlabel=\"alpha (0 = baseline)\",\n",
    "    vmin=0, vmax=1,\n",
    "    cbar_label=\"follow rate\",\n",
    ")\n",
    "\n",
    "plot_metric_heatmap(\n",
    "    reward_pivot,\n",
    "    ax=axes[1],\n",
    "    title=\"response quality by type & alpha\",\n",
    "    xlabel=\"alpha (0 = baseline)\",\n",
    "    fmt=\".1f\",\n",
    "    cbar_label=\"reward\",\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f270b",
   "metadata": {},
   "source": [
    "### Which instructions benefit from steering?\n",
    "\n",
    "We use `plot_comparison_bars` to compare baseline vs steered performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_by_type = instr_summary[instr_summary[\"alpha\"] == 0.0].set_index(\"instruction_type\")\n",
    "best_alpha = 10.0\n",
    "steered_by_type = instr_summary[instr_summary[\"alpha\"] == best_alpha].set_index(\"instruction_type\")\n",
    "\n",
    "comparison_by_type = pd.DataFrame({\n",
    "    \"instruction_type\": baseline_by_type.index,\n",
    "    \"follow_delta\": steered_by_type[\"follow_rate\"].values - baseline_by_type[\"follow_rate\"].values,\n",
    "    \"reward_delta_scaled\": (steered_by_type[\"mean_reward\"].values - baseline_by_type[\"mean_reward\"].values) / 3,\n",
    "}).sort_values(\"follow_delta\", ascending=False)\n",
    "\n",
    "plot_comparison_bars(\n",
    "    comparison_by_type,\n",
    "    metric_cols=[\"follow_delta\", \"reward_delta_scaled\"],\n",
    "    group_col=\"instruction_type\",\n",
    "    title=f\"change from baseline (alpha={best_alpha})\",\n",
    "    ylabel=\"delta from baseline\",\n",
    "    colors=[\"steelblue\", \"coral\"],\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d6a53",
   "metadata": {},
   "source": [
    "### Efficiency Frontier\n",
    "\n",
    "We can overlay a Pareto frontier on the scatter plot using `plot_pareto_frontier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d437bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# filter to non-baseline for scatter\n",
    "pasta_summary = summary[summary[\"config\"] != \"baseline\"]\n",
    "baseline_row = summary[summary[\"config\"] == \"baseline\"].iloc[0] if len(summary[summary[\"config\"] == \"baseline\"]) > 0 else None\n",
    "\n",
    "# plot scatter with tradeoff\n",
    "plot_tradeoff_scatter(\n",
    "    pasta_summary,\n",
    "    x_metric=\"strict_prompt_acc\",\n",
    "    y_metric=\"mean_reward\",\n",
    "    color_col=\"alpha\",\n",
    "    label_col=\"config\",\n",
    "    baseline_row=baseline_row,\n",
    "    ax=ax,\n",
    "    title=\"quality–compliance tradeoff with pareto frontier\",\n",
    "    xlabel=\"strict prompt accuracy\",\n",
    "    ylabel=\"mean reward score\",\n",
    ")\n",
    "\n",
    "# overlay Pareto frontier\n",
    "plot_pareto_frontier(\n",
    "    pasta_summary,\n",
    "    x_metric=\"strict_prompt_acc\",\n",
    "    y_metric=\"mean_reward\",\n",
    "    ax=ax,\n",
    "    maximize_x=True,\n",
    "    maximize_y=True,\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26a0fb",
   "metadata": {},
   "source": [
    "## Takeaways\n",
    "\n",
    "PASTA steering can improve instruction following, but the optimal alpha depends on the acceptable quality trade-off. For this model and task, moderate steering (α ∈ [10, 20]) typically offers the best balance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

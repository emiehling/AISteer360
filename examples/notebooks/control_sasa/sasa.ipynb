{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c47dafb6",
   "metadata": {},
   "source": [
    "<img src=\"./sasa.png\" alt=\"SASA method\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "(Image from Ko et al., 2025)\n",
    "\n",
    "# SASA\n",
    "\n",
    "**Paper**: [Large Language Models Can Become Strong Self-Detoxifiers](https://openreview.net/pdf?id=jY5oml9fe9)\n",
    "\n",
    "**Authors**: Ching-Yun Ko, Pin-Yu Chen, Payel Das, Youssef Mroueh, Soham Dan, Georgios Kollias, Subhajit Chaudhury, Tejaswini Pedapati, Luca Daniel\n",
    "\n",
    "SASA (self-disciplined autoregressive sampling) is an output steering method, enabling the users to perform controlled decoding given any desirable value attributes. \n",
    "\n",
    "SASA leverages the contextual representations from an LLM to learn linear subspaces from labeled data, e.g. characterizing toxic v.s. non-toxic output in analytical forms. When auto-completing a response token-by-token, SASA dynamically tracks the margin of the current output to steer the generation away from the toxic subspace, by adjusting the autoregressive sampling strategy. \n",
    "\n",
    "In this demo, we show how SASA can be used to reduce the toxicity of sentences generated by an LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deced0ae",
   "metadata": {},
   "source": [
    "## Method parameters\n",
    "\n",
    "| parameter           | type            | description                                                                                                           |\n",
    "| ------------------- | --------------- | --------------------------------------------------------------------------------------------------------------------- |\n",
    "| `beta`              | `float`         | Scaling coefficient for value redistribution. Must be non-negative.                                                   |\n",
    "| `wv_path`           | `Optional[str]` | Path to a saved steering-vector tensor. Must end with `.pt` if provided.                                              |\n",
    "| `gen_wv_data_path`  | `Optional[str]` | Path to the value dataset, e.g. sentences with labeled toxicity.                                                      |\n",
    "| `gen_wv_length`     | `Optional[int]` | Maximum number of samples used for preparing SASA steering if `wv_path` does not exist.                               |\n",
    "| `gen_wv_batch_size` | `Optional[int]` | Batch size used for preparing SASA steering if `wv_path` does not exist. Must be non-negative if `wv_path` is `None`. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366ede45",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc00bab",
   "metadata": {},
   "source": [
    "If running this from a Google Colab notebook, please uncomment the following cell to install the toolkit. The following block is not necessary if running this notebook from a virtual environment where the package has already been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d82cb804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/IBM/AISteer360.git\n",
    "# %cd AISteer360"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca625ae",
   "metadata": {},
   "source": [
    "The following authentication steps may be necessary to access any gated models (after being granted access by Hugging Face). Uncomment the following if you need to log in to the Hugging Face Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a57ff10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install python-dotenv\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "\n",
    "# load_dotenv()\n",
    "# token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "# from huggingface_hub import login\n",
    "# login(token=token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc2e967",
   "metadata": {},
   "source": [
    "## Example: Steering for reduced toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28570abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from aisteer360.algorithms.core.steering_pipeline import SteeringPipeline\n",
    "from aisteer360.algorithms.output_control.sasa.control import SASA\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "MODEL_NAME = \"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb1a27",
   "metadata": {},
   "source": [
    "### Downloading data\n",
    "\n",
    "By default, the toxicity subspace is constructed using the Jigsaw dataset from Kaggle. To use `jigsaw_unintended_bias` you can either download it manually from Kaggle (https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data) or run the following cell using the Kaggle API (https://www.kaggle.com/docs/api). Either way, all files should be extracted to one folder, e.g. `'./tmp/Jigsaw_data/all_data.csv'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae96a7e",
   "metadata": {},
   "source": [
    "#### Automated download instructions (run this if you haven't manually downloaded the dataset)\n",
    "\n",
    "To access your Kaggle token (for downloading data using the API tool), first sign in at [kaggle.com](https://www.kaggle.com). Then:\n",
    "- Click your profile photo -> \"Your Profile\" -> \"Settings\"\n",
    "- Scroll to API and click \"Create New Token\"\n",
    "- Your browser immediately downloads `kaggle.json`\n",
    "\n",
    "Place the json in the kaggle directory in root (typically `~/.config/kaggle/`) and execute the following script. \n",
    "\n",
    "**Note**: If you encounter an error 403 (permission error), please ensure that you have clicked \"Join the competition\" under the \"Data\" tab on the dataset homepage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3b145f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /tmp/tmpw5ijoeit\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (25.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.11/site-packages (25.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.11/site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in ./.venv/lib/python3.11/site-packages (0.45.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in ./.venv/lib/python3.11/site-packages (1.7.4.5)\n",
      "Requirement already satisfied: bleach in ./.venv/lib/python3.11/site-packages (from kaggle) (6.2.0)\n",
      "Requirement already satisfied: certifi>=14.05.14 in ./.venv/lib/python3.11/site-packages (from kaggle) (2025.8.3)\n",
      "Requirement already satisfied: charset-normalizer in ./.venv/lib/python3.11/site-packages (from kaggle) (3.4.3)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.11/site-packages (from kaggle) (3.10)\n",
      "Requirement already satisfied: protobuf in ./.venv/lib/python3.11/site-packages (from kaggle) (6.32.1)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./.venv/lib/python3.11/site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in ./.venv/lib/python3.11/site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.11/site-packages (from kaggle) (2.32.5)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in ./.venv/lib/python3.11/site-packages (from kaggle) (80.9.0)\n",
      "Requirement already satisfied: six>=1.10 in ./.venv/lib/python3.11/site-packages (from kaggle) (1.17.0)\n",
      "Requirement already satisfied: text-unidecode in ./.venv/lib/python3.11/site-packages (from kaggle) (1.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.11/site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: urllib3>=1.15.1 in ./.venv/lib/python3.11/site-packages (from kaggle) (2.5.0)\n",
      "Requirement already satisfied: webencodings in ./.venv/lib/python3.11/site-packages (from kaggle) (0.5.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m ensurepip --upgrade\n",
    "!{sys.executable} -m pip install --upgrade pip setuptools wheel\n",
    "!{sys.executable} -m pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17b3a026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /u/erikmiehling/.config/kaggle/kaggle.json'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading jigsaw-unintended-bias-in-toxicity-classification.zip to tmp/Jigsaw_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 723M/723M [00:00<00:00, 2.70GB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os, glob, zipfile, shutil, pandas as pd\n",
    "from pathlib import Path\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "DATA_DIR = Path(\"tmp/Jigsaw_data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "api = KaggleApi(); api.authenticate()\n",
    "api.competition_download_files(\n",
    "    \"jigsaw-unintended-bias-in-toxicity-classification\",\n",
    "    path=str(DATA_DIR),\n",
    "    force=True,\n",
    "    quiet=False\n",
    ")\n",
    "\n",
    "zip_path = glob.glob(str(DATA_DIR / \"*.zip\"))[0]\n",
    "with zipfile.ZipFile(zip_path) as z:\n",
    "    z.extractall(DATA_DIR)\n",
    "\n",
    "train = pd.read_csv(DATA_DIR / \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR / \"test.csv\")\n",
    "\n",
    "label_paths = [\n",
    "    p for p in (\n",
    "        DATA_DIR / \"test_public_expanded.csv\",\n",
    "        DATA_DIR / \"test_private_expanded.csv\",\n",
    "        DATA_DIR / \"test_labels.csv\"\n",
    "    ) if p.exists()\n",
    "]\n",
    "if label_paths:\n",
    "    lbl = pd.concat([pd.read_csv(p) for p in label_paths])\n",
    "    test = test.merge(lbl[[\"id\", \"toxicity\"]], on=\"id\", how=\"left\")\n",
    "\n",
    "out_csv = DATA_DIR / \"all_data.csv\"\n",
    "pd.concat([train, test]).to_csv(out_csv, index=False)\n",
    "\n",
    "# cleanup\n",
    "os.remove(zip_path)\n",
    "for p in DATA_DIR.iterdir():\n",
    "    if p.resolve() != out_csv.resolve():\n",
    "        (p.unlink() if p.is_file() else shutil.rmtree(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3b6979",
   "metadata": {},
   "source": [
    "### Creating the control\n",
    "\n",
    "SASA requires contructing the value subspace prior to the steering. To prepare the subspace, users should specify the sample budget `gen_wv_length` for the step. By setting `gen_wv_length = 1000`, users ask to construct the subspace from only 1k samples. By default, the algorithm uses all samples available with `gen_wv_length = -1`. The parameter `gen_wv_batch_size` represents the batch size used during this step. Users may also adjust it according to their computational resources.\n",
    "Below, `beta` is a positive scalar that represents the steering strength, with `0` replicating the original decoding behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3edc40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sasa = SASA(\n",
    "    beta=10,\n",
    "    gen_wv_length=100,\n",
    "    gen_wv_batch_size=8,\n",
    "    gen_wv_data_path=\"tmp/Jigsaw_data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5497005",
   "metadata": {},
   "source": [
    "If value subspace is available, users can skip the above parameters (`beta`, `gen_wv_length`, `gen_wv_data_path`) and instead specifiy the path to the subspace via `wv_path`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea4d08e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sasa = SASA(\n",
    "#     beta=10,\n",
    "#     wv_path='./tmp/steer_wv.pt',  \n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db28dc65",
   "metadata": {},
   "source": [
    "### Creating the steering pipeline\n",
    "\n",
    "We create a `SteeringPipeline` with the `SASA` control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86f0d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sasa_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[sasa],\n",
    "    device_map=\"cuda\",\n",
    "    hf_model_kwargs={\"low_cpu_mem_usage\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d80d96",
   "metadata": {},
   "source": [
    "Next we steer the pipeline (under the single SASA control). Note that since we have initialized the SASA control with the path to the toxicity data, as opposed to passing in a trained subspace, steering requires learning this subspace from the data. This is resource-heavy step (GPU required)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7426e0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SASA steer (wv)......\n",
      "Dataset found in: tmp/Jigsaw_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dccstor/principled_ai/users/erikmiehling/AISteer360/aisteer360/algorithms/output_control/sasa/control.py:177: DtypeWarning: Columns (32,36) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataset = pd.read_csv(os.path.join(self.gen_wv_data_path, \"all_data.csv\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are overall 68271 positive sentences and 29049 negative sentences.\n",
      "Generating wv via 70 positive sentences and 30 negative sentences.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding POS: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:00<00:00, 42.39it/s]\n",
      "Embedding NEG: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 52.24it/s]\n"
     ]
    }
   ],
   "source": [
    "sasa_pipeline.steer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586cf2cc",
   "metadata": {},
   "source": [
    "After steering, inference can now be run on the pipeline for a given prompt. We define a prompt that attempts to induce toxic behavior in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f035bf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"I had someone throw me to the ground and say: \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ac936f",
   "metadata": {},
   "source": [
    "Passing this prompt into the unsteered (base) model demonstrates a toxic continuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0a0f24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response (baseline):\n",
      "\n",
      "\u00a0\"Hey, you're going to get this on your hands. I'm not gonna take it.\"\n",
      "And that was what happened,\" said Dr. John J. Fenton, a professor of medicine at Columbia University School for Health Care Policy. \"He grabbed my hand with his fist in front of me like he wanted to punch me out of there.\"\n",
      "There are many factors driving these incidents such as poor hygiene or lack of medical training. And some doctors may have been unaware of them before\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "inputs = tokenizer(PROMPT, return_tensors=\"pt\")\n",
    "input_ids = inputs.input_ids\n",
    "attention_mask = inputs.attention_mask\n",
    "\n",
    "gen_params = {\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"temperature\": 0.6,\n",
    "    \"top_p\": 0.9,\n",
    "    \"do_sample\": True,\n",
    "    \"repetition_penalty\": 1.05,\n",
    "}\n",
    "\n",
    "baseline_outputs = model.generate(\n",
    "    **inputs.to(model.device), \n",
    "    **gen_params\n",
    ")\n",
    "\n",
    "print(\"\\nResponse (baseline):\\n\")\n",
    "print(tokenizer.decode(baseline_outputs[0][len(inputs['input_ids'][0]):], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a27bbe7",
   "metadata": {},
   "source": [
    "Compare this with the response of the base model when steered using SASA (via the steering pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "932882b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response (SASA):\n",
      "\n",
      "\u00a0\"Hey, you're going to get this over with. You don't know what happened in that house.\"\n",
      "What did I do? What was my response? How could a person have said something like that when they were so angry at an individual for being \"violent\"? Was it because of their race or ethnicity? Did I feel any guilt about anything happening here? And how can anyone be held responsible if there is no evidence whatsoever linking them to violence against others?\" If only we would ask\n"
     ]
    }
   ],
   "source": [
    "steered_output_ids = sasa_pipeline.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    runtime_kwargs={},\n",
    "    **gen_params,\n",
    ")\n",
    "\n",
    "print(\"\\nResponse (SASA):\\n\")\n",
    "print(tokenizer.decode(steered_output_ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828201b3",
   "metadata": {},
   "source": [
    "Lastly, note that the beta parameter dictates the strength of the steering, and can thus be adjusted to control the degree of toxicity suppression in the generated response (importantly without having to relearn the subspace)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d3095cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SASA steer (wv)......\n",
      "\n",
      "Response (beta=0):\n",
      "\n",
      "\u00a0\"You're a rapist, you should be arrested. You are being punished for your crimes.\"\n",
      "So I said \"Yeah, that's right. If it was just my fault then we would have been together forever,\" but he laughed at me like that and told me not to worry about anything else because if this is what happens in life they will make sure nothing ever comes of it anyway so all bets were off until after school on Wednesday morning when our kids got home from work or dinner having\n"
     ]
    }
   ],
   "source": [
    "sasa = SASA(\n",
    "    beta=0,\n",
    "    wv_path='./tmp/steer_wv.pt',  # we just saved the subspace in the preparation steps above\n",
    ")\n",
    "\n",
    "sasa_pipeline = SteeringPipeline(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    controls=[sasa],\n",
    "    device_map=\"cpu\",\n",
    "    hf_model_kwargs={\"low_cpu_mem_usage\": True},\n",
    ")\n",
    "\n",
    "sasa_pipeline.steer()\n",
    "\n",
    "original_output_ids = sasa_pipeline.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    runtime_kwargs={},\n",
    "    **gen_params,\n",
    ")\n",
    "\n",
    "print(f\"\\nResponse (beta=0):\\n\")\n",
    "print(tokenizer.decode(original_output_ids[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
